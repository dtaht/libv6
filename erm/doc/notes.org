* For casual readers
This started off good, and I am appending my notes as I go along. I should
probably timestamp them...

* There are ~10 global variables
* Everything is optimized for vector processing
** aligned to natural boundaries
** always in tables large enough for a prefetch or dma overrun
* Everything is a table - with frequently accessed types in their own table indexed separately
** all classification is on the way in
** No pointers as much as feasible (I am going to be lazy and violate this a few times)
* C bitfields are used where appropriate, masks elsewhere
** Very small values everywhere - think hardware or 16 bit microcontroller
* Code sharing is maximized
but certain things MAY be precompiled for certain strides.
* Internationalization will be enabled
** But string sharing across modules is hard
* C11 features like structure return are used extensively
** Often structures are returned instead of using the stack
In the case of NEON and SSE we can slam the structures into existing registers.
In the case of hardware certain things end up in registers anyway.
Yes, structures are a PITA in regular C, and hard on the compiler and code author.
* trap handler
I am thinking more and more that instead of a classic goto err thing, we should
define and have a trap handler instead. For example, in the future there might 
be more and different ae encodings than we know how to handle.

* Ipv4 vs IPv6
It is making more and more sense to have disjoint ipv4 and ipv6 tables
The metadata remains the same, but the other index is referenced from an
altenate starting point. How to do this?
* Direct calls to library functions are wrapped
** Because at some point I'm going to violate the std ABI
There will be a few globals kept always in registers
** Register Set essentially looks like this
FLAGS
TRAPS
BASEOFFSET
V6ADDRS
V4ADDRS
V6META
V4META
INTERFACES
ROUTERS
ROUTES

* popcount rather than hashes
Originally I was going to use a hasher. popcnt has a great property 
in that if two popcounts are different, we know things are different.

The algorithm is becoming more similar to a classic skip list 
every day. My intent is to do generational garbage collection
on a tick (either per packet or time based).

* Plenty of registers

Experimentation with the e-gcc C compiler showed that the
registers  identified  as  requiring  “Callee  Saved”  (22  of  them)
are only available if the special word “register” is prepended
to local  variable declarations.  The four registers  identified as
“Reserved for constants” are not allocated and hence not used
in any C code fragments we inspected.
* Merging routes
** popcount is hamming distance
so it is guaranteed to go down as you seach /64 /63 /62

* TODO memoized queries

Multiple query types are essentially memoizable - 

* Multiple Parallallea

Well, what do you do when you run out of space 64,000 routes?
You start up a garuntted to be disjoint set on another cpu,
much like being generational, you send a generation onwards
with a clear filter between them. One clear filter is:

prefix/plen - throw all possible matches against this 
prefix less than plen into another table entirely

* Disjoint portions of the data set
* truth

* Combiner for routes, using the hamming distance
and triggering the garbage collector at each step outwards.

append combined routes with a distance to the next possible route

we establish a combining state, where we see that as a possibility
looking back at the routes we had

*** with an internal compressed routing table we can further optimize
and only export to the kernel "the solution", which in many cases degrades
to just the retracted routes and a default route.

We can also spread out existing route announcements sanely across the compressed
table over a longer interval in the routing announcements

and one day, just announce the aggregated routes as "true", and treat
the arrival of a subset route (with, perhaps a different metric) as the 
onset of uncertainty.

* Compression and eq and volume changes

Thinking aoubt this as an audio problem, with a very low hz is helpful

* plugin traffic generator
* plugin traffic monitor
* plugin wifi stats
** plugin for other stuff

* Tests

Test performance of insert/delete/update/search mechanisms

* Ringbuffer - found one here for c11 (pthreads)

https://github.com/rmind/ringbuf/tree/master/src

but I still want *my* ringbuffer which was a header file that took fixed length quantities and 
used the mmap trick, and had a few other features like high and low watermarks.

https://github.com/stv0g/c11-queues.git looks better. Still - fixed length 
quantities, generated at compile time, would be better, as well as a high/low watermark
feature to find balance, and the tests at least, are written for x86 only.

* TODO I still need to look over librcu.

* TODO Get a reasonably generic get_cycles routine.

* New C99 trick of the month %zd

%zd does the right thing for where %ld and %d are different.
%zu as well.

There's support in gcc and clang and glib and newlib

not sure if it is in musl

* An API begins to emerge

This whole thing is a bit much for ddpd, but the table management
gets simpler the more I abstract things.

split_prefix
dump_prefixes

* macaddr_t table

One of my dreams has long been to implement the rotating mac hash first described
to me by fred baker 4+ years ago. It can try to create a perfect hash, too.

* While I'm at it, printf abuse

%A Address
%P Prefix
%M Macaddr

Then I can pass a string like

pass("%P via %A via %A over %M", prefix, address, nexthop, macaddr);

and not have to copy OR format those variables;

Or write a more specialized one.

* FMA - floating point metrics

The missing "4th" additive field could be replaced by a user specified one

And I'm dying to use a FMA somewhere - the idea of multiplying a result by a
smoothing factor that is actually floating point is a cool idea.

Also we need to use saturating arithmetic carefully.

* Partition based on plen for city-scale routing

As the only 64k bounds that are likely to be exceeded is the total number of addresses
or routes, we can easily partition off a separate virtual co-processor to handle
that, and still share data on interfaces, next hops, and so on.

You can find nearly any division that works, and split off the work as it
happens

fc::/7
172.26.16.0/16
172.26.26.0/16

all represent clear partitions of work.

We can have a number of virtual processors much larger than the actual number of
processors and swap them in and out as needed.

You can, in general, always do this, even if you don't actually have to.

* combining pop and plen

If I made plen always negative a plen of 0 = 0, -128 = ?
I want to distinguish between a source specific and non source specific route

// https://gcc.gnu.org/onlinedocs/gcc/_005f_005fatomic-Builtins.html#g_t_005f_005fatomic-Builtins
// Built-in Function: type __atomic_fetch_xor (type *ptr, type val, int memorder)

* Interruptable garbage collection

At any point a garbage collection attempt can be interrupted, discarded, and
retried. As opposed to "stop the world". 

gc also tries to keep track of how much work is left, in case stopping the
world is needed.

* Merge sort

It is inefficient to insert each new route and metric one at a time, when
up to 80 arrive in a single packet. Instead, all routes are staged, partitioned,
then merged.

* Native endian

I would rather like to use native endianess, converting on the way in and out
as a means to make memcmp-like operations mildly easier to think about.

* partitioner

pushing the partitioning into netlink would be nice -
"give me only things that affect these subnets".

* rules extension for sets
in-set
aorr(a,b,c,d) action
aand

* Strides and minimal table wandering instructions

Originally I thought I'd generate one codebase per function, indexing off of the
globally reserved register for that function.

as I think about it there are a limited number of strides (4,8,16,64?) and truth
values, and so we are probably better off with one generated function per stride
on some architectures, and use an index register per se' more directly, with
post-autoincrement, where available.

this also lines up well with thumb (bottom 8 registers are 16 bit instructions),
adapteva (same), and on x86_64 it's an extra byte per higher reg, also.

I honestly can't remember if the idx register concept survived past 386 at the moment.

* For_each_bla

no: 
stagger(cmd,op,timeval);
stagger(cmd,op,timeval) {
whichcores(mask);
multicast(cmd,random,op,timeval)
}
cast

* Use select profiligately
actually, epoll would be better

* --protocol-extensions
ae for aggregation
ae lying - gradually increase the metric of a smaller route while holding the
aggregate low until the listening router has a brief phase of expiring the route
personally
and also announcing via normal ae that aggregated route
unicast hello
hello with stats
wait for hellos
udp-lite for route transfers - we have a basic fq problem in that we want 
our command channel for heartbeats and a data channel for route transfers

We *could* listen on another port. But udp-lite is "just there".

* BPF filter on interfaces? One netlink socket per interface?
* Snoop on traffic? Count routes that I got?

Even with unicast route transfers we can listen in on a raw socket on wifi
tcpdump ip6 proto udp port 6696 or ( proto udplite port 6696) source address
that's not me. 

parasitic

Can we turn off udp checksumming? Can we use raw checksumming? Can we just
grab stuff at the mac80211 layer giving us the qos fields?

Can we deeply inspect the packet? (42?) ? look for a hash? Get the ipclass?
can we at least filter out all the nonipv4 traffic?

char *opt;
opt = "eth0";
setsockopt(sd, SOL_SOCKET, SO_BINDTODEVICE, opt, 4);

My application is running on CentOS 5.5. I'm using raw socket to send data:

sd = socket(AF_INET, SOCK_RAW, IPPROTO_RAW);
if (sd < 0) {
  // Error
}
const int opt_on = 1;
rc = setsockopt(m_SocketDescriptor, IPPROTO_IP, IP_HDRINCL, &opt_on, sizeof(opt_on));
if (rc < 0) {
  close(sd);
  // Error
}
struct sockaddr_in sin;
memset(&sin, 0, sizeof(sin));
sin.sin_family = AF_INET;
sin.sin_addr.s_addr = my_ip_address;

if (sendto(m_SocketDescriptor, DataBuffer, (size_t)TotalSize, 0, (struct sockaddr *)&sin, sizeof(struct sockaddr)) < 0)  {
  close(sd);
  // Error
}

#define SERVERPORT 5555
...
struct ifreq ifr;


/* Create the socket */
sd = socket(AF_INET, SOCK_STREAM, 0);
if (sd < 0) 
{
    printf("Error in socket() creation - %s", strerror(errno));
}

/* Bind to eth1 interface only - this is a private VLAN */
memset(&ifr, 0, sizeof(ifr));
snprintf(ifr.ifr_name, sizeof(ifr.ifr_name), "eth1");
if ((rc = setsockopt(sd, SOL_SOCKET, SO_BINDTODEVICE, (void *)&ifr, sizeof(ifr))) < 0)
{
    perror("Server-setsockopt() error for SO_BINDTODEVICE");
    printf("%s\n", strerror(errno));
    close(sd);
    exit(-1);
}

/* bind to an address */
memset(&serveraddr, 0x00, sizeof(struct sockaddr_in));
serveraddr.sin_family = AF_INET;
serveraddr.sin_port = htons(SERVERPORT);
serveraddr.sin_addr.s_addr = inet_addr("9.1.2.3");

int rc = bind(sd, (struct sockaddr *)&serveraddr, sizeof(serveraddr));

* Next up popcount!

implemented as a generic routine for all types using C11 generics.
I hope.

Then, off to review the table logic. I may adopt go's convention
for identifying these "registers" here, as they *are* global symbols.

Definately, treating the ipv4 and ipv6 paths as separate is looking like a big win.

* HAT-trie?

https://github.com/malbrain/HatTrie

I would like to plunk some alternative to qsort into mainline babeld.

This might be worthwhile trying

* While I am endlessly appending stuff

Another long stalled out effort was the ipv6 timestamp header.

There was source for this, as I recall... and boy could I use it. I could punt
timestamping to kernels that supported it. And on a p2p link, the header might
pass.


https://www.ietf.org/proceedings/87/slides/slides-87-6man-4.pdf

And another:

http://www.dcs.gla.ac.uk/~dp/pubs/Pezaros-NOMS04-CRC.pdf

* I incidentally hit upon a good way to experiment with babel with less fear

switch the proto I'm using to UDPLITE
* started at popcount
and made a mess of it
* Endian

I really, really, really want to convert endianness on the way in and way out.

But as near as I can tell by leveraging popcount and plen first,
I can just consider > < 

but I need to think about it.

Similarly, memcmp in sse routines

* TODO going back to ddpd

should roll the command line api while fresh on my mind
also - api split prefix/join prefix - uses factory pattern

* TODO EBPF compiler in llvm

What can I make EBPF do?

faster expression for

tcpdump ip6 and '( proto udp or proto 138 )' and offset? portnumber = 6696 and notme

but I actually want to start using extension headers so ip6 chain?


    setsockopt(sock, SOL_SOCKET, SO_ATTACH_FILTER_EBPF, &prog_id, sizeof(prog_id));

* options

define run-yacc =
yacc $(firstword $^)
mv y.tab.c $@
endef

* adapteva notes

e-gcc
ifeq($(CC),e-gcc) 
CFLAGS +=-m1reg-r63 - use -63 for negative constants
CFLAGS +=-falign-loops=8
CFLAGS +=-falign-functions=8
endif

linker, alloc_size, cold,flatten,always inline, malloc, pure, section
gcc compiler directives

--entry=entry

__core_row_
__core_col__
__stack_start__

http://adapteva.com/docs/epiphany_sdk_ref.pdf pg 49

asm(".global __core_row__;");
asm(".set __core_row__,0x20;");
asm(".global __core_col__;");
asm(".set __core_col__,0x24;");

* Idea: Generational kernel tables.

Use 2 specific kernel tables, send all routes into the new one,
then change the rule atomically, flush all routes from the old one.

Dealing with netlink's semantics is a pain in the ass.

IF routes get rejected (pointing somewhere wrong), push them on a local stack
and retry. Ideally the route insertion process is sorted correctly, but who
knows.

* Auto generate flags arguments big and little endian and a means to print them out.

* Aggregating routes

** join? adjecent? I have a better name for this

* x86 notes

** declaring my calloc
For instance,

void* my_calloc(size_t, size_t) __attribute__((alloc_size(1,2)))
void* my_realloc(void*, size_t) __attribute__((alloc_size(2)))
declares that my_calloc returns memory of the size given by the product of parameter 1 and 2 and that my_realloc returns memory of the size given by parameter 2.
** really wanted to violate the C calling convention
and just return a set flags register in several cases


artificial

useful:
https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#Common-Function-Attributes

void* my_alloc1(size_t) __attribute__((assume_aligned(16)))
void* my_alloc2(size_t) __attribute__((assume_aligned(32, 8)))

** cold optimizes for size not speed and puts in a separate section
** ffreestanding or -fno-builtin might shut the compiler up on my printfs
-Wformat
** malloc tells the compiler I can alias stuff
** const is not allowed to read global memory.

and I want to violate that, selectively. Someone stop me!
** hot gets optimized extensively yes!
** flatten
** no_split_stack
** bnd_instrument looks USEFUL for doing bounds checking
fchkp-instrument-marked-only
extern void *
mymalloc (size_t len) __attribute__((returns_nonnull));
** simd is actually a keyword...
simd("mask")

** Way more compiler options than I want

# -mregparm=num
# -msseregparm=
# -mcx16 - have 16 byte cmpxchg
# -msahf
# -mmovbe big endian optimization
# -mcrc32 - used in google's hash
# -mno-align-stringops - don't bother with alignment
# -minline-all-stringops
# -fomit-frame-pointer
# -mfpmath=sse
# sseregparm is a function attribute I can declare. Yea!
# -mno-ieee-fp
# __builtin_types_compatible_p
# profile feedback via: -fprofile-arcs
# __builtin_trap
# __builtin_parity
# __builtin_ffsl find first set long
# __builtin_clzl
# __builtin_bswap64
# https://gcc.gnu.org/onlinedocs/gcc/x86-Function-Attributes.html
# target(abm)

# what does the https://github.com/adobe-flash/crossbridge/blob/master/llvm-gcc-4.2-2.9/gcc/testsuite/gcc.target/i386/fastcall-sseregparm.c
# fast call do?
# no_caller_saved_registers
# void __f () { /* Do something. */; }
# void f () __attribute__ ((weak, alias ("__f")));
* mmap trick

one thing that I keep thinking about using is always using and bit offsets
so that a program can never overrun its personal memory. Example:

 var[ a = ++a & 16] = will endlessly cycle

The other related trick is mmaping things back onto themselves as per the
ringbuffer tick.

We have (so far) a definition that the first location of a table is "special",
in that a compare *could* stop there if it overran. Hmm. What if I made the
"special value" all ones instead of zero?

 while(match != val && val != 0) val[a = ++a & 16];

however in a couple places, I made it less special than that, and this does not
necessarily work well with common post or pre indexed addressing modes.

 my "plan" was to always fold these two comparisons into an SSE reg before
 moving them back into the main unit. But I think ones, rather than zero,
 is the saner out. Now. Which I didn't before. Hmm....

* Idea: run multiple versions in parallel

we could fire off multiple copies of this to cross check each other.

On both the epiphany and parallella at the same time.

shades of the shuttle.
* route transfers over authenticated tcp

using the noise framework, and announcing my public key


* So I looked at the code on arm and parallela

I was expect it to use the pre or post indexed addressing mode. It didn't,
preferring an explicit immediate add. I'd also kind of expected it to fold the
compare into a conditional instruction. Unless I'm not reading this right.

http://www.cs.uregina.ca/Links/class-info/301/ARM-addressing/lecture.html

What I think I am seeing is register indirect addressing with an offset.

-------------------------------------------------------------------------------------
	LDR R0, [R1], #4        R1         ; loads R0 with the word pointed at by R1
					   ; then update the pointer by adding 4
	to R1

  d0:   f89d 3007       ldrb.w  r3, [sp, #7]
  d4:   b2db            uxtb    r3, r3
  d6:   1c5a            adds    r2, r3, #1
  d8:   b2d2            uxtb    r2, r2
  da:   f88d 2007       strb.w  r2, [sp, #7]
  de:   b12b            cbz     r3, ec <tbl_a_tbl_b_roar_warm_bounded_overrun_ex
plicit_match_yield_post+0x2c>
  e0:   6833            ldr     r3, [r6, #0]
  e2:   3604            adds    r6, #4
  e4:   4299            cmp     r1, r3
  e6:   bf08            it      eq
  e8:   3501            addeq   r5, #1
  ea:   e7f1            b.n     d0 <tbl_a_tbl_b_roar_warm_bounded_overrun_explicit_match_yield_post+0x10>


There was a mistake in my original code in taht I used volatile too much, but
to me, damn it, this load is postindexed. I don't care if the compiler is
smarter than me or not, I want one less instruction. 

 2cc:   e5943000        ldr     r3, [r4]
 2d0:   e2844004        add     r4, r4, #4
 2d4:   e0233001        eor     r3, r3, r1
 2d8:   e0855003        add     r5, r5, r3
 2dc:   e2823001        add     r3, r2, #1
 2e0:   e3520000        cmp     r2, #0
 2e4:   e6ef2073        uxtb    r2, r3
 2e8:   1afffff7        bne     2cc <tbl_a_tbl_b_roar_warm_bounded_overrun_explicit_match_yield_xor_post_forced_locals+0x1c>
 
-Os after I killed the wrong registration

 278:   e5962000        ldr     r2, [r6]
 27c:   e2866004        add     r6, r6, #4 // WHY?
 280:   e0222001        eor     r2, r2, r1
 284:   e0855002        add     r5, r5, r2
 288:   e1a02003        mov     r2, r3 // WHY? Does the mov have a side effect?
 28c:   eafffff5        b       268 <tbl_a_tbl_b_roar_warm_bounded_overrun_explicit_match_yield_xor_post_forced_locals+0x18>

Maybe I need to be explicit about it being an array?... no....

.L47:
        adds    r3, r2, #1      @ tmp128, d,
        uxtb    r3, r3  @ d, tmp128
        cbz     r2, .L50        @ d,
        ldr     r2, [r7, r5, lsl #2]    @ *_25, *_25
        adds    r5, r5, #1      @ i, i,
        eors    r2, r2, r0      @, D.4599, *_25, match
        add     r6, r6, r2      @ r, D.4599
        mov     r2, r3  @ d, d
        b       .L47    @

** So... I hit the web and found that it is a frequently missed optimization in gcc. Sigh

https://gcc.gnu.org/bugzilla/show_bug.cgi?id=42612

Given how small this compiles down to in the first place, I got no problem
rewriting it to "do the right thing". Later. All I'd have to do is change the
generated assembly to:

 278:   e5962000        ldr     r2, [r6], #4
 280:   e0222001        eor     r2, r2, r1
 284:   e0855002        add     r5, r5, r2
 288:   e1a02003        mov     r2, r3 // WHY?
 28c:   eafffff5        b       268 <tbl_a_tbl_b_roar_warm_bounded_overrun_explicit_match_yield_xor_post_forced_locals+0x18>

And lest you think I'm being anal, that's a 20% reduction in instructions in the
main loop. And I still don't know why r2 is moving to r3 all the time. Going
from 6 instuctions to 3 seems like a win - and this is essentially the nomatch
operation - revised here - but I suspect eor doesn't generate flags... could
have sworn I could embed a conditional in this tho, on arm

 278:   e5962000        ldr     r2, [r6], #4
 280:   e0222001        eor     r2, r2, r1
 28c:   eafffff5        bne       268 <tbl_a_tbl_b_roar_warm_bounded_overrun_explicit_match_yield_xor_post_forced_locals+0x18>

I guess I'll try fiddling with likely and so on.

and the parallella NEEDS indirect post indexed to work well. Grump.

* IMPORTANT NOTE

And lest you think the above can't possibly work and will overrun memory, the
trick is you always put what you are searching for in a special area at the end
of the table, so you will ALWAYS match (eventually), and you fix it, later, in
the backtracker.

Now, it is likely that this as written will stall a lot, but, we'll see.

H/T to mel, the programmer for this old trick, I think. Maybe I'll use cmov,
I dunno.

* llvm does something closer to the "right thing"

it even does a loop unroll. But I still don't get it.

Anyway. 8 ins for the real search routine, 14 bytes on regular arm

.LBB11_4:                               @   Parent Loop BB11_2 Depth=1
                                        @ =>  This Inner Loop Header: Depth=2
        ldr     r6, [r2], #4
        tst     r0, #255
        eor     r6, r6, r1
        add     r5, r6, r5
        add     r6, r0, #1
        mov     r0, r6
        bne     .LBB11_4


        ldr     r1, [r5], #4
        cmp     r0, r1
        addeq   r6, r6, #1
        and     r1, r4, r6
        add     r4, r4, #1
        tst     r1, #15
        bne     .LBB13_3

.LBB12_3:                               @ %.lr.ph
                                        @   Parent Loop BB12_2 Depth=1
                                        @ =>  This Inner Loop Header: Depth=2
        ldr     r1, [r4], #4
        cmp     r0, r1
        add     r1, r7, #1
        addeq   r5, r5, #1
        tst     r7, #15
        mov     r7, r1
        bne     .LBB12_3

* The preamble got bigger. No like

And I didn't get the pre-increment addressing out of the parallela either.

I did get it out of llvm on the arm, but that is not generating thumb
Maybe either mode is exclusive of thumb?

* Getting rid of the count variable helps

* And an unrolled version may be the winner for small values

Could NOT get the darn thing to vectorize...

addition shouldn't have a dependency that the compiler sees. Adding 1 for
equality comparison
should never overflow... grump
maybe less than rather than and? Definately generates different code

maybe use |?

Maybe load shift | ?

load 64 bits split it apart?

If these core routines were not so important I'd have given up by now.

I think I will just have to write this stuff in pure assembly.

After I get the ability to load up a lot of stuff.

and STILL I want structure return a,b = pointer, result
in registers.

dang it.


char my_char; //normal method
char *my_other_char = (char*)0x6000; //hardcoding addresses

//External memory using section labels
int my_integer SECTION("shared_dram"); //section at 0x8f000000
float my_float SECTION("heap_dram"); //section at 0x8f800000

* __COUNTER__

 could maybe be used to generate trap numbers

* SOURCE_DATE_EPOCH

can be used instead of __DATE__ for repeatable builds

* Token pasting and preprocessing

For all I know I'll end up preprocessing things twice in some cases

#define xstr(s) str(s)
#define str(s) #s
#define foo 4


str (foo)
     → "foo"
xstr (foo)
     → xstr (4)
     → str (4)
     → "4"


* noize vs ssh
Originally I was all hot on using the noize framework but as I've
not found much more than specs, that is cooling. What is so wrong
with ssh?

** Negatives
- need "users" on the remote machine in the right group
- still need to do separate key management to some extent
- faster to announce your public key via some means
- I'd thought to actually use a client-server binary protocol
- have to fork/open a pipe to ssh for input/output
- I'd intended the command parser to run locally and just send
commands (mapped to numbers) over the link.
- Not sure how to make it work with mosh
- binary transport is VERY doable but I get less insight into the transaction
  (tcp) that I wanted. Hmm... after the fact via a netlink filter?
** Postives
- Crypto is someone elses problem
- somewhat more ideally you'd want your own dedic
- much smaller - just ship the shared memory client and quit

** Massive positives
It just works. Fired up init.dbg /testinstance on the other side

ssh germ@172.26.16.16

d@dancer:~/git/libv6/erm/src$ ssh germ@172.26.16.16
warning: Couldn't mmap shared huge page memory in erm_attach_client(): Invalid argument
Deregistering erm instance
warning: Couldn't unmap shared memory in erm_close(): Invalid argument
erm instance detached

I merely slammed a script that called up the instance:

command="/usr/bin/grm.sh",no-port-forwarding,no-X11-forwarding,no-agent-forwarding ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDuXeMnLWmyup6V6Hls63yx/nbg/CrPGMCxtBZriYgAnZ9LZyKuBlKo0l6TppQzouO50qYQQv9pu0eoF4sZeIX7STepDX/9EH96Hsd/8ThQt7BrzteSJVqaIE41pon/wXN/oNcVNscYHcb0YwqY3Q+/4NkkXu/QYOrDhafw8eJb7LAdyKupaGiXmN6SkSQHFS2hHcuCl+ZaRAJCUBG+1eNd+w23C6A6Q7/OUppoS1EeSpA1bdnkUDeTTQ6JA72dvoBxDSsVVfva8Zvszz7qz5E7NiLWP+tH6IYahl9FA04oQoOxjbtl+eRI9vqyTxVAwUNkx4Wrazl6AgKmv+l4n3AJ d@dancer

Am not sure if I can get away with no controlling pty. Can dropbear accept this syntax?

Put that user in the erm group... (currently 84). and boom. Good stuff. Others
are denied.

baduser@par:~/.ssh$ germ -a /testinstance
fail: Couldn't open shared memory - aborting in erm_attach_client(): Permission denied
baduser@par:~/.ssh$ germ -a /testinstance

* what was so wrong with everything as a file?

I was asking myself just this past week. While abusing mmap.

d@dancer:/dev/shm/erma$ find . -print
.
./memory
./arch
./mach
./flags
./machine
./ocpu
./ocpu/0
./ocpu/1
./ocpu/2
./ocpu/3
./ocpu/4
./ocpu/5
./ocpu/6
./ocpu/7
./ocpu/8
./ocpu/9
./ocpu/10
./ocpu/11
./ocpu/12
./ocpu/13
./ocpu/14
./ocpu/15
./formats
./stats
./daemons
./daemons/hnet
./daemons/udhcp6c
./daemons/udhcp
./daemons/dnsmasq
./daemons/odhcpd
./daemons/tabeld
./daemons/kernel
./daemons/boot
./daemons/static
./addresses
./routes
./routes/hnet
./routes/udhcp6c
./routes/udhcp
./routes/dnsmasq
./routes/odhcpd
./routes/tabeld
./routes/kernel
./routes/boot
./routes/static
./interfaces
./rules
./self
./cpu
./cpu/0
./cpu/1
./cpu/2
./cpu/3

I could just layer rsync over it, too. A file changes, you use inotify.
write, delete, mv.

/erm/lookup/myaddress ?

* For that matter what was wrong with rsync?

Can that be used to transfer route tables (sorted by pop | plen and compressed
to 16 bit indexes)? effeciently?

* icc does have a 30 day eval

And does generate my desired cmove instruction.

 330:   44 8d 1c 4a             lea    (%rdx,%rcx,2),%r11d
 334:   44 0f 44 ce             cmove  %esi,%r9d
 338:   45 8d 53 06             lea    0x6(%r11),%r10d
 33c:   45 03 c1                add    %r9d,%r8d
 33f:   41 83 e2 07             and    $0x7,%r10d
 343:   41 8d 43 ff             lea    -0x1(%r11),%eax
 347:   41 89 c1                mov    %eax,%r9d
 34a:   45 85 d0                test   %r10d,%r8d
 34d:   74 20                   je     36f <tbl_a_tbl_b_roar_match_firsthit+0x7f>

and the unrolled version looks like a winner across the board on intel.
Admittedly on a very inefficient load of a 16 bit value.

 19f:   45 0f b7 5e 04          movzwl 0x4(%r14),%r11d
 1a4:   45 0f 44 d7             cmove  %r15d,%r10d
 1a8:   45 33 c9                xor    %r9d,%r9d
 1ab:   33 d2                   xor    %edx,%edx
 1ad:   45 3b e3                cmp    %r11d,%r12d
 1b0:   41 0f b7 5e 06          movzwl 0x6(%r14),%ebx
 1b5:   45 0f 44 cf             cmove  %r15d,%r9d
 1b9:   44 3b e3                cmp    %ebx,%r12d
 1bc:   bb 00 00 00 00          mov    $0x0,%ebx
 1c1:   41 0f 44 df             cmove  %r15d,%ebx
 1c5:   41 03 ca                add    %r10d,%ecx
 1c8:   44 03 cb                add    %ebx,%r9d
 1cb:   45 33 d2                xor    %r10d,%r10d
 1ce:   41 0f b7 5e 08          movzwl 0x8(%r14),%ebx

benchmarks await!

* SSE return sucks. Maybe I can stick the args in local regs?

	call	fool_compiler2	#
	movl	$.LC0, %edi	#,
	call	puts	#
	movl	48(%rsp), %edx	# d.f,
	movl	16(%rsp), %esi	# c.f,
	leaq	64(%rsp), %rdi	#, tmp131
	movdqa	32(%rsp), %xmm1	# d.a,
	movdqa	(%rsp), %xmm0	# c.a,
	call	fool_compiler2	#
	movl	$.LC0, %edi	#,
	call	puts	#
	movl	48(%rsp), %esi	# d.f,
	movl	16(%rsp), %edi	# c.f,
	movdqa	32(%rsp), %xmm1	# d.a,
	movdqa	(%rsp), %xmm0	# c.a,
	call	fool_compiler3	#

in the typical case I inline the core stuff here but

* Back to vectorization

Last time I did this I used icc to pry apart what I needed.

https://software.intel.com/sites/default/files/m/4/8/8/2/a/31848-CompilerAutovectorizationGuide.pdf

An elemental function is a regular function, which can be invoked either on scalar
arguments or on array elements in parallel. You define an elemental function by adding
“__declspec(vector)” (on Windows*) and “__attribute__((vector))” (on Linux*)
before 

You need to declare it BEFORE the function signature...

need to get good about restrict also.

>icl /c /Qvec-report2 vectorFunc.cpp

and the magic was:

icl /c /Qguide gap.cpp

whic no longer exists...


% ifort -vec-report3 atest.f

There is an Intel compiler option '-vec-report=<n>', to generate a diagnostic
report about vectorization. Use it to see why the compiler vectorizes certain
loops and doesn't vectorize others. The report level can be selected by setting
the flag to any number from 0 to 7. See the ifort, icc or icpc man page for
details. This is an example of the report:

#pragma omp simd [clause...]

e safelen cl

y
http://www.nersc.gov/users/computational-systems/edison/programming/vectorization/


* regcall: https://software.intel.com/en-us/node/522787

__m128, __m128i, __m128d


__attribute__((regcall)) foo (int I, int j); 

-regcall if you want it on always

* Well, I succeeded. 

But Still think the code generated is suboptimal.

00000000000003b0 <tbl_a_tbl_b_roar_match_firsthit_vvector>:
 3b0:   41 54                   push   %r12
 3b2:   41 55                   push   %r13
 3b4:   56                      push   %rsi
 3b5:   49 89 fd                mov    %rdi,%r13
 3b8:   49 89 f4                mov    %rsi,%r12
 3bb:   66 0f ef ff             pxor   %xmm7,%xmm7
 3bf:   f3 0f 6f 1d 00 00 00    movdqu 0x0(%rip),%xmm3        # 3c7 <tbl_a_tbl_b_roar_match_firsthit_vvector+0x17>
 3c6:   00 
 3c7:   41 0f b7 7d 00          movzwl 0x0(%r13),%edi
 3cc:   f3 41 0f 7e 14 24       movq   (%r12),%xmm2
 3d2:   f3 41 0f 7e 4c 24 08    movq   0x8(%r12),%xmm1
 3d9:   66 0f 6e c7             movd   %edi,%xmm0
 3dd:   66 0f 70 e0 00          pshufd $0x0,%xmm0,%xmm4
 3e2:   66 0f 61 d7             punpcklwd 61 cf             punpcklwd %xmm7,%xmm1
 3ea:   66 0f 76 d4             pcmpeqd %xmm4,%xmm2
 3ee:   66 0f 76 e1             pcmpeqd %xmm1,%xmm4
 3f2:   66 0f db d3             pand   %xmm3,%xmm2
 3f6:   66 0f fe fa             paddd  %xmm2,%xmm7
 3fa:   66 0f db e3             pand   %xmm3,%xmm4
 3fe:   66 0f fe fc             paddd  %xmm4,%xmm7
 402:   66 0f 6f ef             movdqa %xmm7,%xmm5
 406:   66 0f 73 dd 08          psrldq $0x8,%xmm5
 40b:   66 0f fe fd             paddd  %xmm5,%xmm7
 40f:   66 0f 6f f7             movdqa %xmm7,%xmm6
 413:   66 0f 73 d6 20          psrlq  $0x20,%xmm6
 418:   66 0f fe fe             paddd  %xmm6,%xmm7
 41c:   66 0f 7e fa             movd   %xmm7,%edx
 420:   ff ca                   dec    %edx


what should happen is we splay the match value into one 8 reg
the "to be matched" into another...
the and into another.
then you mask out 0x1 in each for the comparsons
Then you do a horizontal add of everything
and bring it back into the main register.


Maybe that's sse3 only and I didn't enable it?

_mm_hadd_epi16

_mm_hadds_pi16

** Guided Auto Parallelization

The Intel compiler includes a Guided Auto Parallelization (GAP) feature that can help analyze source code and generate advice on how to obtain better performance. In particular, GAP will suggest code changes or compiler options that will lead to better vectorized code. GAP may optionally allow the user to take advantage of the auto-parallelization capability that can generate multithreaded code for independent loop iterations; however, developers are encouraged to use explicit thread parallelism through mechanisms like OpenMP.

The GAP feature can be accessed by adding the -guide option, which takes an optional =# parameter, where # can be a number between 1 and 4 with 1 being the lowest level of guidance and 4 (the default) being the most advanced level of guidance.  The compiler will print a GAP report to stderr or it can be redirected to a file with the -guide-file=filename option, which will send the output to the file name filename, or the -guide-file-append=filename option, which will append to the specified file.  The GAP analysis can be targeted to a specific file, function, or source line with the -guide-opt=specification option. Refer to the compiler man pages or documentation for details on this option.

https://computing.llnl.gov/?set=code&page=intel_vector

**  3c6:   00 
 3c7:   41 0f b7 7d 00          movzwl 0x0(%r13),%edi
 3cc:   f3 41 0f 7e 14 24       movq   (%r12),%xmm2
 3d2:   f3 41 0f 7e 4c 24 08    movq   0x8(%r12),%xmm1
 3d9:   66 0f 6e c7             movd   %edi,%xmm0
 3dd:   66 0f 70 e0 00          pshufd $0x0,%xmm0,%xmm4
 3e2:   66 0f 61 d7             punpcklwd %xmm7,%xmm2
 3e6:   66 0f 61 cf             punpcklwd %xmm7,%xmm1
 3ea:   66 0f 76 d4             pcmpeqd %xmm4,%xmm2
 3ee:   66 0f 76 e1             pcmpeqd %xmm1,%xmm4
 3f2:   66 0f db d3             pand   %xmm3,%xmm2
 3f6:   66 0f fe fa             paddd  %xmm2,%xmm7
 3fa:   66 0f db e3             pand   %xmm3,%xmm4
 3fe:   66 0f fe fc             paddd  %xmm4,%xmm7
 402:   66 0f 6f ef             movdqa %xmm7,%xmm5
 406:   66 0f 73 dd 08          psrldqpsrldq $0x8,%xmm5
 40b:   66 0f fe fd             paddd  %xmm5,%xmm7
 40f:   66 0f 6f f7             movdqa %xmm7,%xmm6
 413:   66 0f 73 d6 20          psrlq  $0x20,%xmm6
 418:   66 0f fe fe             paddd  %xmm6,%xmm7
 41c:   66 0f 7e fa             movd   %xmm7,%edx
 420:   ff ca                   dec    %edx
 422:   41 0f b7 34 24          movzwl (%r12),%esi

**  It seems likely that I am still smarter than the compiler.

Except that I'm not able to outsmart it. I can do the damn job in
*2* registers. Not 7. And in 11ins. Admitted phadd is 
pretty slow, but...

movq match reg1
mov 01 into reg2
(can't I just put them all in at the same time)?
splay match (might stash it somewhere in a more optimized version
splay 01 reg2
PCMPEQW reg1, tbl b* # destroying reg1
pandw reg2, reg1 # destroying reg2

* horizontal add reg2, reg2 # with itself:
PHADDW xmm1, xmm1 gives me 4 arrays of 16
PHADD xmm1, xmm1 gives me 2 vecs arrays of 32
shuffle
phadd xmm1 xmm1
grab the result

I can even do this 256 bytes wide now with avx.

there's probably another way to do this.

* I can get away from the mask and do a lookup
in cases where I will have 1 and only 1 match

movq match reg1
splay reg1
stash reg3 reg1 ; keep a copy
loop: PCMPEQW reg1, tbl b* # destroying reg1
padd4 ; darn, can overflow if I have more matches - but I wont in most cases
this is used
; 64 bits I got now adays!
shuffle the two vectors into 64 bits
mov to ecx
** 6 sse ins!
; lookup the result - if it has any ones we had a hit and have to backtrack
test
beq have_hit
; otherwise we motor on...
inc tablb b + 16; can get interleaved elsewhere (after the padd)
mov reg1 reg3; can get interleaved elsewhere (with the padd)
goto loop:

have_hit:
; if I interleave, we need to backtrack 32, not 16
to turn the result in a lookuptable... - let's see...

* mov fix

shufle reg1, match 

* aarch Neon version
vld  reg2 *tbl a; load all lanes with lowest value
vld; reg0 copy the lowest 16 bits
loop: vld; reg1 tblb
cmp: reg1, reg1, reg0
padd4 reg1, reg1, reg1 - padd the result - again: can overflow, bad
shufl - get result into one reg? (maybe can skip)
store to main unit
test
beq have_hit
vld reg1 tblb preindexed
goto loop: 

* Overflow is scary

I think the and version is just safer. overflows make me nervous,
and we get a value from 0,1,2,4, | the same shifted << 32

so the backtracker just has to pull the high value from the
unit if there's a miss in the early dword.

* And gcc doesn't like the pragma

BUT does successfully vectorize and use a load and only 3 regs.

Yea!

 30d:   66 0f 61 c9             punpcklwd %xmm1,%xmm1
 311:   66 0f 70 c9 00          pshufd $0x0,%xmm1,%xmm1
 316:   66 0f 75 0e             pcmpeqw (%rsi),%xmm1
 31a:   66 0f db 0d 00 00 00    pand   0x0(%rip),%xmm1        # 322 <tbl_a_tbl_b_roar_match_firsthit_vvvector+0x32>
 321:   00 
 322:   66 0f 6f c1             movdqa %xmm1,%xmm0
 326:   66 0f 61 ca             punpcklwd %xmm2,%xmm1
 32a:   66 0f 69 c2             punpckhwd %xmm2,%xmm0
 32e:   66 0f fe c8             paddd  %xmm0,%xmm1
 332:   66 0f 6f d1             movdqa %xmm1,%xmm2
 336:   66 0f 6f c1             movdqa %xmm1,%xmm0
 33a:   66 0f 73 da 08          psrldq $0x8,%xmm2
 33f:   66 0f fe c2             paddd  %xmm2,%xmm0
 343:   66 0f 6f c8             movdqa %xmm0,%xmm1
 347:   66 0f 73 d9 04          psrldq $0x4,%xmm1
 34c:   66 0f fe c1             paddd  %xmm1,%xmm0
 350:   66 0f 7e c2             movd   %xmm0,%edx
 354:   83 ea 01                sub    $0x1,%edx
 357:   eb 0b                   jmp    364 <tbl_a_tbl_b_roar_match_firsthit_vvvector+0x74>

* Future Notes for auto vectorization

Loops should not be pointers, but array indexes.
There needs to be a clear "for" component with a fixed
termination.

It is totally feasible to use my & and freerunning tricks but the compiler
doesn't like it.

* In an orgy of excess

still not there... 

search 255 entries in a single go, not vectored

 0f     b6 c8                   movzbl %al,%ecx
 1f8    66 39 7c 4b fe          cmp    %di,-0x2(%rbx,%rcx,2)
 200:   0f 94 c1                sete   %cl
 203:   01 ca                   add    %ecx,%edx
 205:   04 01                   add    $0x1,%al
 207:   75 ef                   jne    1f8 <tbl_a_tbl_b_roar_match_freerun_vvectormaybe+0x18>
 209:   0f b7 73 fe             movzwl -0x2(%rbx),%esi
 20d:   0f b6 d2                movzbl %dl,%edx
 210:   e8 00 00 00 00          callq  215 <tbl_a_tbl_b_roar_match_freerun_vvectormaybe+0x35>


SOMETIMES_INLINE tbl_b PO(roar_match_freerun_vvectormaybe)(tbl_a* a, tbl_b* B_ALIGNED b)
{
  b = __builtin_assume_aligned (b, 16);
  unsigned char d;
  unsigned char r = -1;
  tbl_a match = *a;
  b--;
  #pragma simd
  for(d = 1; d != 0; d++) // I love writing infinite looking loops even tho icc will complain
	  r += (match == b[d]);
  r = RESULT(r, a, b);

  return *b;
}


// like so much here, this is not correct code
SOMETIMES_INLINE tbl_b PO(roar_match_freerun_vvectormaybe)(tbl_a* a, tbl_b* B_ALIGNED b)
{
  b = __builtin_assume_aligned (b, 16);
  unsigned char d;
  unsigned char r = -1;
  tbl_a match = *a;
  b--;
  #pragma simd
  for(d = 1; d ; d++) // I love writing infinite looking loops even tho icc will complain
	  r += (match == b[d]);
  r = RESULT(r, a, b);

  return *b;
}


00000000000001e0 <tbl_a_tbl_b_roar_match_freerun_vvectormaybe>:
 1e0:   53                      push   %rbx
 1e1:   0f b7 3f                movzwl (%rdi),%edi
 1e4:   48 89 f3                mov    %rsi,%rbx
 1e7:   ba ff ff ff ff          mov    $0xffffffff,%edx
 1ec:   b8 01 00 00 00          mov    $0x1,%eax
 1f1:   0f 1f 80 00 00 00 00    nopl   0x0(%rax)
 1f8:   0f b6 c8                movzbl %al,%ecx
 1fb:   66 39 7c 4b fe          cmp    %di,-0x2(%rbx,%rcx,2)
 200:   0f 94 c1                sete   %cl
 203:   01 ca                   add    %ecx,%edx
 205:   04 01                   add    $0x1,%al
 207:   75 ef                   jne    1f8 <tbl_a_tbl_b_roar_match_freerun_vvectormaybe+0x18>
 209:   0f b7 73 fe             movzwl -0x2(%rbx),%esi
 20d:   0f b6 d2                movzbl %dl,%edx
 210:   e8 00 00 00 00          callq  215 <tbl_a_tbl_b_roar_match_freerun_vvectormaybe+0x35>
 215:   0f b7 43 fe             movzwl -0x2(%rbx),%eax
 219:   5b                      pop    %rbx
 21a:   c3                      retq   
 21b:   0f 1f 44 00 00          nopl   0x0(%rax,%rax,1)

** But only this will vectorize
// like so much here, this is not correct code
SOMETIMES_INLINE tbl_b PO(roar_match_freerun_vvectormaybe2)(tbl_a* a, tbl_b* B_ALIGNED b)
{
  b = __builtin_assume_aligned (b, 16);
  unsigned char d;
  unsigned char r = -1;
  tbl_a match = *a;
  #pragma simd
  for(d = 0; d < 255 ; d++) // I love writing infinite looking loops even tho icc will complain
	  r += (match == b[d]);
  r = RESULT(r, a, b);

  return *b;
}

which it does a pretty huge complete unroll on which
is probably not what I want either

// Still I like constraining my space this way:

0000000000000220 <tbl_a_tbl_b_roar_match_freerun_vvectorno_firstexit>:
 220:   53                      push   %rbx
 221:   0f b7 3f                movzwl (%rdi),%edi
 224:   48 89 f3                mov    %rsi,%rbx
 227:   b8 01 00 00 00          mov    $0x1,%eax
 22c:   0f 1f 40 00             nopl   0x0(%rax)
 230:   0f b6 d0                movzbl %al,%edx
 233:   66 39 7c 53 fe          cmp    %di,-0x2(%rbx,%rdx,2)
 238:   0f 94 c2                sete   %dl
 23b:   83 c0 01                add    $0x1,%eax
 23e:   83 ea 01                sub    $0x1,%edx
 241:   84 c2                   test   %al,%dl
 243:   75 eb                   jne    230 <tbl_a_tbl_b_roar_match_freerun_vvectorno_firstexit+0x10>
 245:   0f b7 73 fe             movzwl -0x2(%rbx),%esi
 249:   0f b6 d2                movzbl %dl,%edx
 24c:   e8 00 00 00 00          callq  251 <tbl_a_tbl_b_roar_match_freerun_vvectorno_firstexit+0x31>
 251:   0f b7 43 fe             movzwl -0x2(%rbx),%eax
 255:   5b                      pop    %rbx
 256:   c3                      retq   
 257:   66 0f 1f 84 00 00 00    nopw   0x0(%rax,%rax,1)

// like so much here, this is not correct code
SOMETIMES_INLINE tbl_b PO(roar_match_freerun_vvectorno_firstexit)(tbl_a* a, tbl_b* B_ALIGNED b)
{
  b = __builtin_assume_aligned (b, 16);
  unsigned char d;
  unsigned char r = -1;
  tbl_a match = *a;
  b--;
  #pragma simd
  for(d = 1; d & r; d++) // I love writing infinite looking loops even tho icc will complain
	  r += (match == b[d]);
  r = RESULT(r, a, b);

  return *b;
}

and if I need a shorter search, constraining d = 256 - 16


In file included from tables_search.c(22):
../includes/tables_search.h(138): error: parallel loop condition operator must be one of <, <=, >, >=, or !=
    for(d = 1; d ; d++) // I love writing infinite looking loops even tho icc will complain

