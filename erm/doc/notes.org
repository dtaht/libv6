* For casual readers
This started off good, and I am appending my notes as I go along. I should
probably timestamp them...

* There are ~10 global variables
* Everything is optimized for vector processing
** aligned to natural boundaries
** always in tables large enough for a prefetch or dma overrun
* Everything is a table - with frequently accessed types in their own table indexed separately
** all classification is on the way in
** No pointers as much as feasible (I am going to be lazy and violate this a few times)
* C bitfields are used where appropriate, masks elsewhere
** Very small values everywhere - think hardware or 16 bit microcontroller
* Code sharing is maximized
but certain things MAY be precompiled for certain strides.
* Internationalization will be enabled
** But string sharing across modules is hard
* C11 features like structure return are used extensively
** Often structures are returned instead of using the stack
In the case of NEON and SSE we can slam the structures into existing registers.
In the case of hardware certain things end up in registers anyway.
Yes, structures are a PITA in regular C, and hard on the compiler and code author.
* trap handler
I am thinking more and more that instead of a classic goto err thing, we should
define and have a trap handler instead. For example, in the future there might 
be more and different ae encodings than we know how to handle.

* Ipv4 vs IPv6
It is making more and more sense to have disjoint ipv4 and ipv6 tables
The metadata remains the same, but the other index is referenced from an
altenate starting point. How to do this?
* Direct calls to library functions are wrapped
** Because at some point I'm going to violate the std ABI
There will be a few globals kept always in registers
** Register Set essentially looks like this
FLAGS
TRAPS
BASEOFFSET
V6ADDRS
V4ADDRS
V6META
V4META
INTERFACES
ROUTERS
ROUTES

* popcount rather than hashes
Originally I was going to use a hasher. popcnt has a great property 
in that if two popcounts are different, we know things are different.

The algorithm is becoming more similar to a classic skip list 
every day. My intent is to do generational garbage collection
on a tick (either per packet or time based).

* Plenty of registers

Experimentation with the e-gcc C compiler showed that the
registers  identified  as  requiring  “Callee  Saved”  (22  of  them)
are only available if the special word “register” is prepended
to local  variable declarations.  The four registers  identified as
“Reserved for constants” are not allocated and hence not used
in any C code fragments we inspected.
* Merging routes
** popcount is hamming distance
so it is guaranteed to go down as you seach /64 /63 /62

* TODO memoized queries

Multiple query types are essentially memoizable - 

* Multiple Parallallea

Well, what do you do when you run out of space 64,000 routes?
You start up a garuntted to be disjoint set on another cpu,
much like being generational, you send a generation onwards
with a clear filter between them. One clear filter is:

prefix/plen - throw all possible matches against this 
prefix less than plen into another table entirely

* Disjoint portions of the data set
* truth

* Combiner for routes, using the hamming distance
and triggering the garbage collector at each step outwards.

append combined routes with a distance to the next possible route

we establish a combining state, where we see that as a possibility
looking back at the routes we had

*** with an internal compressed routing table we can further optimize
and only export to the kernel "the solution", which in many cases degrades
to just the retracted routes and a default route.

We can also spread out existing route announcements sanely across the compressed
table over a longer interval in the routing announcements

and one day, just announce the aggregated routes as "true", and treat
the arrival of a subset route (with, perhaps a different metric) as the 
onset of uncertainty.

* Compression and eq and volume changes

Thinking aoubt this as an audio problem, with a very low hz is helpful

* plugin traffic generator
* plugin traffic monitor
* plugin wifi stats
** plugin for other stuff

* Tests

Test performance of insert/delete/update/search mechanisms

* Ringbuffer - found one here for c11 (pthreads)

https://github.com/rmind/ringbuf/tree/master/src

but I still want *my* ringbuffer which was a header file that took fixed length quantities and 
used the mmap trick, and had a few other features like high and low watermarks.

https://github.com/stv0g/c11-queues.git looks better. Still - fixed length 
quantities, generated at compile time, would be better, as well as a high/low watermark
feature to find balance, and the tests at least, are written for x86 only.

* TODO I still need to look over librcu.

* TODO Get a reasonably generic get_cycles routine.

* New C99 trick of the month %zd

%zd does the right thing for where %ld and %d are different.
%zu as well.

There's support in gcc and clang and glib and newlib

not sure if it is in musl

* An API begins to emerge

This whole thing is a bit much for ddpd, but the table management
gets simpler the more I abstract things.

split_prefix
dump_prefixes

* macaddr_t table

One of my dreams has long been to implement the rotating mac hash first described
to me by fred baker 4+ years ago. It can try to create a perfect hash, too.

* While I'm at it, printf abuse

%A Address
%P Prefix
%M Macaddr

Then I can pass a string like

pass("%P via %A via %A over %M", prefix, address, nexthop, macaddr);

and not have to copy OR format those variables;

Or write a more specialized one.

* FMA - floating point metrics

The missing "4th" additive field could be replaced by a user specified one

And I'm dying to use a FMA somewhere - the idea of multiplying a result by a
smoothing factor that is actually floating point is a cool idea.

Also we need to use saturating arithmetic carefully.

* Partition based on plen for city-scale routing

As the only 64k bounds that are likely to be exceeded is the total number of addresses
or routes, we can easily partition off a separate virtual co-processor to handle
that, and still share data on interfaces, next hops, and so on.

You can find nearly any division that works, and split off the work as it
happens

fc::/7
172.26.16.0/16
172.26.26.0/16

all represent clear partitions of work.

We can have a number of virtual processors much larger than the actual number of
processors and swap them in and out as needed.

You can, in general, always do this, even if you don't actually have to.

* combining pop and plen

If I made plen always negative a plen of 0 = 0, -128 = ?
I want to distinguish between a source specific and non source specific route

// https://gcc.gnu.org/onlinedocs/gcc/_005f_005fatomic-Builtins.html#g_t_005f_005fatomic-Builtins
// Built-in Function: type __atomic_fetch_xor (type *ptr, type val, int memorder)

* Interruptable garbage collection

At any point a garbage collection attempt can be interrupted, discarded, and
retried. As opposed to "stop the world". 

gc also tries to keep track of how much work is left, in case stopping the
world is needed.

* Merge sort

It is inefficient to insert each new route and metric one at a time, when
up to 80 arrive in a single packet. Instead, all routes are staged, partitioned,
then merged.

* Native endian

I would rather like to use native endianess, converting on the way in and out
as a means to make memcmp-like operations mildly easier to think about.

* partitioner

pushing the partitioning into netlink would be nice -
"give me only things that affect these subnets".

* rules extension for sets
in-set
aorr(a,b,c,d) action
aand

* Strides and minimal table wandering instructions

Originally I thought I'd generate one codebase per function, indexing off of the
globally reserved register for that function.

as I think about it there are a limited number of strides (4,8,16,64?) and truth
values, and so we are probably better off with one generated function per stride
on some architectures, and use an index register per se' more directly, with
post-autoincrement, where available.

this also lines up well with thumb (bottom 8 registers are 16 bit instructions),
adapteva (same), and on x86_64 it's an extra byte per higher reg, also.

I honestly can't remember if the idx register concept survived past 386 at the moment.

* For_each_bla

no: 
stagger(cmd,op,timeval);
stagger(cmd,op,timeval) {
whichcores(mask);
multicast(cmd,random,op,timeval)
}
cast

* Use select profiligately
actually, epoll would be better

* --protocol-extensions
ae for aggregation
ae lying - gradually increase the metric of a smaller route while holding the
aggregate low until the listening router has a brief phase of expiring the route
personally
and also announcing via normal ae that aggregated route
unicast hello
hello with stats
wait for hellos
udp-lite for route transfers - we have a basic fq problem in that we want 
our command channel for heartbeats and a data channel for route transfers

We *could* listen on another port. But udp-lite is "just there".

* BPF filter on interfaces? One netlink socket per interface?
* Snoop on traffic? Count routes that I got?

Even with unicast route transfers we can listen in on a raw socket on wifi
tcpdump ip6 proto udp port 6696 or ( proto udplite port 6696) source address
that's not me. 

parasitic

Can we turn off udp checksumming? Can we use raw checksumming? Can we just
grab stuff at the mac80211 layer giving us the qos fields?

Can we deeply inspect the packet? (42?) ? look for a hash? Get the ipclass?
can we at least filter out all the nonipv4 traffic?

char *opt;
opt = "eth0";
setsockopt(sd, SOL_SOCKET, SO_BINDTODEVICE, opt, 4);

My application is running on CentOS 5.5. I'm using raw socket to send data:

sd = socket(AF_INET, SOCK_RAW, IPPROTO_RAW);
if (sd < 0) {
  // Error
}
const int opt_on = 1;
rc = setsockopt(m_SocketDescriptor, IPPROTO_IP, IP_HDRINCL, &opt_on, sizeof(opt_on));
if (rc < 0) {
  close(sd);
  // Error
}
struct sockaddr_in sin;
memset(&sin, 0, sizeof(sin));
sin.sin_family = AF_INET;
sin.sin_addr.s_addr = my_ip_address;

if (sendto(m_SocketDescriptor, DataBuffer, (size_t)TotalSize, 0, (struct sockaddr *)&sin, sizeof(struct sockaddr)) < 0)  {
  close(sd);
  // Error
}

#define SERVERPORT 5555
...
struct ifreq ifr;


/* Create the socket */
sd = socket(AF_INET, SOCK_STREAM, 0);
if (sd < 0) 
{
    printf("Error in socket() creation - %s", strerror(errno));
}

/* Bind to eth1 interface only - this is a private VLAN */
memset(&ifr, 0, sizeof(ifr));
snprintf(ifr.ifr_name, sizeof(ifr.ifr_name), "eth1");
if ((rc = setsockopt(sd, SOL_SOCKET, SO_BINDTODEVICE, (void *)&ifr, sizeof(ifr))) < 0)
{
    perror("Server-setsockopt() error for SO_BINDTODEVICE");
    printf("%s\n", strerror(errno));
    close(sd);
    exit(-1);
}

/* bind to an address */
memset(&serveraddr, 0x00, sizeof(struct sockaddr_in));
serveraddr.sin_family = AF_INET;
serveraddr.sin_port = htons(SERVERPORT);
serveraddr.sin_addr.s_addr = inet_addr("9.1.2.3");

int rc = bind(sd, (struct sockaddr *)&serveraddr, sizeof(serveraddr));

* Next up popcount!

implemented as a generic routine for all types using C11 generics.
I hope.

Then, off to review the table logic. I may adopt go's convention
for identifying these "registers" here, as they *are* global symbols.

Definately, treating the ipv4 and ipv6 paths as separate is looking like a big win.

* HAT-trie?

https://github.com/malbrain/HatTrie

I would like to plunk some alternative to qsort into mainline babeld.

This might be worthwhile trying

* While I am endlessly appending stuff

Another long stalled out effort was the ipv6 timestamp header.

There was source for this, as I recall... and boy could I use it. I could punt
timestamping to kernels that supported it. And on a p2p link, the header might
pass.


https://www.ietf.org/proceedings/87/slides/slides-87-6man-4.pdf

And another:

http://www.dcs.gla.ac.uk/~dp/pubs/Pezaros-NOMS04-CRC.pdf

* I incidentally hit upon a good way to experiment with babel with less fear

switch the proto I'm using to UDPLITE
* started at popcount
and made a mess of it
* Endian

I really, really, really want to convert endianness on the way in and way out.

But as near as I can tell by leveraging popcount and plen first,
I can just consider > < 

but I need to think about it.

Similarly, memcmp in sse routines

* TODO going back to ddpd

should roll the command line api while fresh on my mind
also - api split prefix/join prefix - uses factory pattern

* TODO EBPF compiler in llvm

What can I make EBPF do?

faster expression for

tcpdump ip6 and '( proto udp or proto 138 )' and offset? portnumber = 6696 and notme

but I actually want to start using extension headers so ip6 chain?


    setsockopt(sock, SOL_SOCKET, SO_ATTACH_FILTER_EBPF, &prog_id, sizeof(prog_id));

* options

define run-yacc =
yacc $(firstword $^)
mv y.tab.c $@
endef

* adapteva notes

e-gcc
ifeq($(CC),e-gcc) 
CFLAGS +=-m1reg-r63 - use -63 for negative constants
CFLAGS +=-falign-loops=8
CFLAGS +=-falign-functions=8
endif

linker, alloc_size, cold,flatten,always inline, malloc, pure, section
gcc compiler directives

--entry=entry

__core_row_
__core_col__
__stack_start__

http://adapteva.com/docs/epiphany_sdk_ref.pdf pg 49

asm(".global __core_row__;");
asm(".set __core_row__,0x20;");
asm(".global __core_col__;");
asm(".set __core_col__,0x24;");

* Idea: Generational kernel tables.

Use 2 specific kernel tables, send all routes into the new one,
then change the rule atomically, flush all routes from the old one.

Dealing with netlink's semantics is a pain in the ass.

IF routes get rejected (pointing somewhere wrong), push them on a local stack
and retry. Ideally the route insertion process is sorted correctly, but who
knows.

* Auto generate flags arguments big and little endian and a means to print them out.

* Aggregating routes

** join? adjecent? I have a better name for this

* x86 notes

** declaring my calloc
For instance,

void* my_calloc(size_t, size_t) __attribute__((alloc_size(1,2)))
void* my_realloc(void*, size_t) __attribute__((alloc_size(2)))
declares that my_calloc returns memory of the size given by the product of parameter 1 and 2 and that my_realloc returns memory of the size given by parameter 2.
** really wanted to violate the C calling convention
and just return a set flags register in several cases


artificial

useful:
https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#Common-Function-Attributes

void* my_alloc1(size_t) __attribute__((assume_aligned(16)))
void* my_alloc2(size_t) __attribute__((assume_aligned(32, 8)))

** cold optimizes for size not speed and puts in a separate section
** ffreestanding or -fno-builtin might shut the compiler up on my printfs
-Wformat
** malloc tells the compiler I can alias stuff
** const is not allowed to read global memory.

and I want to violate that, selectively. Someone stop me!
** hot gets optimized extensively yes!
** flatten
** no_split_stack
** bnd_instrument looks USEFUL for doing bounds checking
fchkp-instrument-marked-only
extern void *
mymalloc (size_t len) __attribute__((returns_nonnull));
** simd is actually a keyword...
simd("mask")

** Way more compiler options than I want

# -mregparm=num
# -msseregparm=
# -mcx16 - have 16 byte cmpxchg
# -msahf
# -mmovbe big endian optimization
# -mcrc32 - used in google's hash
# -mno-align-stringops - don't bother with alignment
# -minline-all-stringops
# -fomit-frame-pointer
# -mfpmath=sse
# sseregparm is a function attribute I can declare. Yea!
# -mno-ieee-fp
# __builtin_types_compatible_p
# profile feedback via: -fprofile-arcs
# __builtin_trap
# __builtin_parity
# __builtin_ffsl find first set long
# __builtin_clzl
# __builtin_bswap64
# https://gcc.gnu.org/onlinedocs/gcc/x86-Function-Attributes.html
# target(abm)

# what does the https://github.com/adobe-flash/crossbridge/blob/master/llvm-gcc-4.2-2.9/gcc/testsuite/gcc.target/i386/fastcall-sseregparm.c
# fast call do?
# no_caller_saved_registers
# void __f () { /* Do something. */; }
# void f () __attribute__ ((weak, alias ("__f")));
* mmap trick

one thing that I keep thinking about using is always using and bit offsets
so that a program can never overrun its personal memory. Example:

 var[ a = ++a & 16] = will endlessly cycle

The other related trick is mmaping things back onto themselves as per the
ringbuffer tick.

We have (so far) a definition that the first location of a table is "special",
in that a compare *could* stop there if it overran. Hmm. What if I made the
"special value" all ones instead of zero?

 while(match != val && val != 0) val[a = ++a & 16];

however in a couple places, I made it less special than that, and this does not
necessarily work well with common post or pre indexed addressing modes.

 my "plan" was to always fold these two comparisons into an SSE reg before
 moving them back into the main unit. But I think ones, rather than zero,
 is the saner out. Now. Which I didn't before. Hmm....

* Idea: run multiple versions in parallel

we could fire off multiple copies of this to cross check each other.

On both the epiphany and parallella at the same time.

shades of the shuttle.
* route transfers over authenticated tcp

using the noise framework, and announcing my public key


* So I looked at the code on arm and parallela

I was expect it to use the pre or post indexed addressing mode. It didn't,
preferring an explicit immediate add. I'd also kind of expected it to fold the
compare into a conditional instruction. Unless I'm not reading this right.

http://www.cs.uregina.ca/Links/class-info/301/ARM-addressing/lecture.html

What I think I am seeing is register indirect addressing with an offset.

-------------------------------------------------------------------------------------
	LDR R0, [R1], #4        R1         ; loads R0 with the word pointed at by R1
					   ; then update the pointer by adding 4
	to R1

  d0:   f89d 3007       ldrb.w  r3, [sp, #7]
  d4:   b2db            uxtb    r3, r3
  d6:   1c5a            adds    r2, r3, #1
  d8:   b2d2            uxtb    r2, r2
  da:   f88d 2007       strb.w  r2, [sp, #7]
  de:   b12b            cbz     r3, ec <tbl_a_tbl_b_roar_warm_bounded_overrun_ex
plicit_match_yield_post+0x2c>
  e0:   6833            ldr     r3, [r6, #0]
  e2:   3604            adds    r6, #4
  e4:   4299            cmp     r1, r3
  e6:   bf08            it      eq
  e8:   3501            addeq   r5, #1
  ea:   e7f1            b.n     d0 <tbl_a_tbl_b_roar_warm_bounded_overrun_explicit_match_yield_post+0x10>


There was a mistake in my original code in taht I used volatile too much, but
to me, damn it, this load is postindexed. I don't care if the compiler is
smarter than me or not, I want one less instruction. 

 2cc:   e5943000        ldr     r3, [r4]
 2d0:   e2844004        add     r4, r4, #4
 2d4:   e0233001        eor     r3, r3, r1
 2d8:   e0855003        add     r5, r5, r3
 2dc:   e2823001        add     r3, r2, #1
 2e0:   e3520000        cmp     r2, #0
 2e4:   e6ef2073        uxtb    r2, r3
 2e8:   1afffff7        bne     2cc <tbl_a_tbl_b_roar_warm_bounded_overrun_explicit_match_yield_xor_post_forced_locals+0x1c>
 
-Os after I killed the wrong registration

 278:   e5962000        ldr     r2, [r6]
 27c:   e2866004        add     r6, r6, #4 // WHY?
 280:   e0222001        eor     r2, r2, r1
 284:   e0855002        add     r5, r5, r2
 288:   e1a02003        mov     r2, r3 // WHY? Does the mov have a side effect?
 28c:   eafffff5        b       268 <tbl_a_tbl_b_roar_warm_bounded_overrun_explicit_match_yield_xor_post_forced_locals+0x18>

Maybe I need to be explicit about it being an array?... no....

.L47:
        adds    r3, r2, #1      @ tmp128, d,
        uxtb    r3, r3  @ d, tmp128
        cbz     r2, .L50        @ d,
        ldr     r2, [r7, r5, lsl #2]    @ *_25, *_25
        adds    r5, r5, #1      @ i, i,
        eors    r2, r2, r0      @, D.4599, *_25, match
        add     r6, r6, r2      @ r, D.4599
        mov     r2, r3  @ d, d
        b       .L47    @

** So... I hit the web and found that it is a frequently missed optimization in gcc. Sigh

https://gcc.gnu.org/bugzilla/show_bug.cgi?id=42612

Given how small this compiles down to in the first place, I got no problem
rewriting it to "do the right thing". Later. All I'd have to do is change the
generated assembly to:

 278:   e5962000        ldr     r2, [r6], #4
 280:   e0222001        eor     r2, r2, r1
 284:   e0855002        add     r5, r5, r2
 288:   e1a02003        mov     r2, r3 // WHY?
 28c:   eafffff5        b       268 <tbl_a_tbl_b_roar_warm_bounded_overrun_explicit_match_yield_xor_post_forced_locals+0x18>

And lest you think I'm being anal, that's a 20% reduction in instructions in the
main loop. And I still don't know why r2 is moving to r3 all the time. Going
from 6 instuctions to 3 seems like a win - and this is essentially the nomatch
operation - revised here - but I suspect eor doesn't generate flags... could
have sworn I could embed a conditional in this tho, on arm

 278:   e5962000        ldr     r2, [r6], #4
 280:   e0222001        eor     r2, r2, r1
 28c:   eafffff5        bne       268 <tbl_a_tbl_b_roar_warm_bounded_overrun_explicit_match_yield_xor_post_forced_locals+0x18>

I guess I'll try fiddling with likely and so on.

and the parallella NEEDS indirect post indexed to work well. Grump.

* IMPORTANT NOTE

And lest you think the above can't possibly work and will overrun memory, the
trick is you always put what you are searching for in a special area at the end
of the table, so you will ALWAYS match (eventually), and you fix it, later, in
the backtracker.

Now, it is likely that this as written will stall a lot, but, we'll see.

H/T to mel, the programmer for this old trick, I think. Maybe I'll use cmov,
I dunno.

* llvm does something closer to the "right thing"

it even does a loop unroll. But I still don't get it.

Anyway. 8 ins for the real search routine, 14 bytes on regular arm

.LBB11_4:                               @   Parent Loop BB11_2 Depth=1
                                        @ =>  This Inner Loop Header: Depth=2
        ldr     r6, [r2], #4
        tst     r0, #255
        eor     r6, r6, r1
        add     r5, r6, r5
        add     r6, r0, #1
        mov     r0, r6
        bne     .LBB11_4


        ldr     r1, [r5], #4
        cmp     r0, r1
        addeq   r6, r6, #1
        and     r1, r4, r6
        add     r4, r4, #1
        tst     r1, #15
        bne     .LBB13_3

.LBB12_3:                               @ %.lr.ph
                                        @   Parent Loop BB12_2 Depth=1
                                        @ =>  This Inner Loop Header: Depth=2
        ldr     r1, [r4], #4
        cmp     r0, r1
        add     r1, r7, #1
        addeq   r5, r5, #1
        tst     r7, #15
        mov     r7, r1
        bne     .LBB12_3

* The preamble got bigger. No like

And I didn't get the pre-increment addressing out of the parallela either.

I did get it out of llvm on the arm, but that is not generating thumb
Maybe either mode is exclusive of thumb?

* Getting rid of the count variable helps

* And an unrolled version may be the winner for small values

Could NOT get the darn thing to vectorize...

addition shouldn't have a dependency that the compiler sees. Adding 1 for
equality comparison
should never overflow... grump
maybe less than rather than and? Definately generates different code

maybe use |?

Maybe load shift | ?

load 64 bits split it apart?

If these core routines were not so important I'd have given up by now.

I think I will just have to write this stuff in pure assembly.

After I get the ability to load up a lot of stuff.

and STILL I want structure return a,b = pointer, result
in registers.

dang it.


char my_char; //normal method
char *my_other_char = (char*)0x6000; //hardcoding addresses

//External memory using section labels
int my_integer SECTION("shared_dram"); //section at 0x8f000000
float my_float SECTION("heap_dram"); //section at 0x8f800000

* __COUNTER__

 could maybe be used to generate trap numbers

* SOURCE_DATE_EPOCH

can be used instead of __DATE__ for repeatable builds

* Token pasting and preprocessing

For all I know I'll end up preprocessing things twice in some cases

#define xstr(s) str(s)
#define str(s) #s
#define foo 4


str (foo)
     → "foo"
xstr (foo)
     → xstr (4)
     → str (4)
     → "4"


* noize vs ssh
Originally I was all hot on using the noize framework but as I've
not found much more than specs, that is cooling. What is so wrong
with ssh?

** Negatives
- need "users" on the remote machine in the right group
- still need to do separate key management to some extent
- faster to announce your public key via some means
- I'd thought to actually use a client-server binary protocol
- have to fork/open a pipe to ssh for input/output
- I'd intended the command parser to run locally and just send
commands (mapped to numbers) over the link.
- Not sure how to make it work with mosh
- binary transport is VERY doable but I get less insight into the transaction
  (tcp) that I wanted. Hmm... after the fact via a netlink filter?
** Postives
- Crypto is someone elses problem
- somewhat more ideally you'd want your own dedic
- much smaller - just ship the shared memory client and quit

** Massive positives
It just works. Fired up init.dbg /testinstance on the other side

ssh germ@172.26.16.16

d@dancer:~/git/libv6/erm/src$ ssh germ@172.26.16.16
warning: Couldn't mmap shared huge page memory in erm_attach_client(): Invalid argument
Deregistering erm instance
warning: Couldn't unmap shared memory in erm_close(): Invalid argument
erm instance detached

I merely slammed a script that called up the instance:

command="/usr/bin/grm.sh",no-port-forwarding,no-X11-forwarding,no-agent-forwarding ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDuXeMnLWmyup6V6Hls63yx/nbg/CrPGMCxtBZriYgAnZ9LZyKuBlKo0l6TppQzouO50qYQQv9pu0eoF4sZeIX7STepDX/9EH96Hsd/8ThQt7BrzteSJVqaIE41pon/wXN/oNcVNscYHcb0YwqY3Q+/4NkkXu/QYOrDhafw8eJb7LAdyKupaGiXmN6SkSQHFS2hHcuCl+ZaRAJCUBG+1eNd+w23C6A6Q7/OUppoS1EeSpA1bdnkUDeTTQ6JA72dvoBxDSsVVfva8Zvszz7qz5E7NiLWP+tH6IYahl9FA04oQoOxjbtl+eRI9vqyTxVAwUNkx4Wrazl6AgKmv+l4n3AJ d@dancer

Am not sure if I can get away with no controlling pty. Can dropbear accept this syntax?

Put that user in the erm group... (currently 84). and boom. Good stuff. Others
are denied.

baduser@par:~/.ssh$ germ -a /testinstance
fail: Couldn't open shared memory - aborting in erm_attach_client(): Permission denied
baduser@par:~/.ssh$ germ -a /testinstance

* what was so wrong with everything as a file?

I was asking myself just this past week. While abusing mmap.

d@dancer:/dev/shm/erma$ find . -print
.
./memory
./arch
./mach
./flags
./machine
./ocpu
./ocpu/0
./ocpu/1
./ocpu/2
./ocpu/3
./ocpu/4
./ocpu/5
./ocpu/6
./ocpu/7
./ocpu/8
./ocpu/9
./ocpu/10
./ocpu/11
./ocpu/12
./ocpu/13
./ocpu/14
./ocpu/15
./formats
./stats
./daemons
./daemons/hnet
./daemons/udhcp6c
./daemons/udhcp
./daemons/dnsmasq
./daemons/odhcpd
./daemons/tabeld
./daemons/kernel
./daemons/boot
./daemons/static
./addresses
./routes
./routes/hnet
./routes/udhcp6c
./routes/udhcp
./routes/dnsmasq
./routes/odhcpd
./routes/tabeld
./routes/kernel
./routes/boot
./routes/static
./interfaces
./rules
./self
./cpu
./cpu/0
./cpu/1
./cpu/2
./cpu/3

I could just layer rsync over it, too. A file changes, you use inotify.
write, delete, mv.

/erm/lookup/myaddress ?

* For that matter what was wrong with rsync?

Can that be used to transfer route tables (sorted by pop | plen and compressed
to 16 bit indexes)? effeciently?

* icc does have a 30 day eval

And does generate my desired cmove instruction.

 330:   44 8d 1c 4a             lea    (%rdx,%rcx,2),%r11d
 334:   44 0f 44 ce             cmove  %esi,%r9d
 338:   45 8d 53 06             lea    0x6(%r11),%r10d
 33c:   45 03 c1                add    %r9d,%r8d
 33f:   41 83 e2 07             and    $0x7,%r10d
 343:   41 8d 43 ff             lea    -0x1(%r11),%eax
 347:   41 89 c1                mov    %eax,%r9d
 34a:   45 85 d0                test   %r10d,%r8d
 34d:   74 20                   je     36f <tbl_a_tbl_b_roar_match_firsthit+0x7f>

and the unrolled version looks like a winner across the board on intel.
Admittedly on a very inefficient load of a 16 bit value.

 19f:   45 0f b7 5e 04          movzwl 0x4(%r14),%r11d
 1a4:   45 0f 44 d7             cmove  %r15d,%r10d
 1a8:   45 33 c9                xor    %r9d,%r9d
 1ab:   33 d2                   xor    %edx,%edx
 1ad:   45 3b e3                cmp    %r11d,%r12d
 1b0:   41 0f b7 5e 06          movzwl 0x6(%r14),%ebx
 1b5:   45 0f 44 cf             cmove  %r15d,%r9d
 1b9:   44 3b e3                cmp    %ebx,%r12d
 1bc:   bb 00 00 00 00          mov    $0x0,%ebx
 1c1:   41 0f 44 df             cmove  %r15d,%ebx
 1c5:   41 03 ca                add    %r10d,%ecx
 1c8:   44 03 cb                add    %ebx,%r9d
 1cb:   45 33 d2                xor    %r10d,%r10d
 1ce:   41 0f b7 5e 08          movzwl 0x8(%r14),%ebx

benchmarks await!

* SSE return sucks. Maybe I can stick the args in local regs?

	call	fool_compiler2	#
	movl	$.LC0, %edi	#,
	call	puts	#
	movl	48(%rsp), %edx	# d.f,
	movl	16(%rsp), %esi	# c.f,
	leaq	64(%rsp), %rdi	#, tmp131
	movdqa	32(%rsp), %xmm1	# d.a,
	movdqa	(%rsp), %xmm0	# c.a,
	call	fool_compiler2	#
	movl	$.LC0, %edi	#,
	call	puts	#
	movl	48(%rsp), %esi	# d.f,
	movl	16(%rsp), %edi	# c.f,
	movdqa	32(%rsp), %xmm1	# d.a,
	movdqa	(%rsp), %xmm0	# c.a,
	call	fool_compiler3	#

in the typical case I inline the core stuff here but

* Back to vectorization

Last time I did this I used icc to pry apart what I needed.

https://software.intel.com/sites/default/files/m/4/8/8/2/a/31848-CompilerAutovectorizationGuide.pdf

An elemental function is a regular function, which can be invoked either on scalar
arguments or on array elements in parallel. You define an elemental function by adding
“__declspec(vector)” (on Windows*) and “__attribute__((vector))” (on Linux*)
before 

You need to declare it BEFORE the function signature...

need to get good about restrict also.

>icl /c /Qvec-report2 vectorFunc.cpp

and the magic was:

icl /c /Qguide gap.cpp

whic no longer exists...


% ifort -vec-report3 atest.f

There is an Intel compiler option '-vec-report=<n>', to generate a diagnostic
report about vectorization. Use it to see why the compiler vectorizes certain
loops and doesn't vectorize others. The report level can be selected by setting
the flag to any number from 0 to 7. See the ifort, icc or icpc man page for
details. This is an example of the report:

#pragma omp simd [clause...]

e safelen cl

y
http://www.nersc.gov/users/computational-systems/edison/programming/vectorization/


* regcall: https://software.intel.com/en-us/node/522787

__m128, __m128i, __m128d


__attribute__((regcall)) foo (int I, int j); 

-regcall if you want it on always

* Well, I succeeded. 

But Still think the code generated is suboptimal.

00000000000003b0 <tbl_a_tbl_b_roar_match_firsthit_vvector>:
 3b0:   41 54                   push   %r12
 3b2:   41 55                   push   %r13
 3b4:   56                      push   %rsi
 3b5:   49 89 fd                mov    %rdi,%r13
 3b8:   49 89 f4                mov    %rsi,%r12
 3bb:   66 0f ef ff             pxor   %xmm7,%xmm7
 3bf:   f3 0f 6f 1d 00 00 00    movdqu 0x0(%rip),%xmm3        # 3c7 <tbl_a_tbl_b_roar_match_firsthit_vvector+0x17>
 3c6:   00 
 3c7:   41 0f b7 7d 00          movzwl 0x0(%r13),%edi
 3cc:   f3 41 0f 7e 14 24       movq   (%r12),%xmm2
 3d2:   f3 41 0f 7e 4c 24 08    movq   0x8(%r12),%xmm1
 3d9:   66 0f 6e c7             movd   %edi,%xmm0
 3dd:   66 0f 70 e0 00          pshufd $0x0,%xmm0,%xmm4
 3e2:   66 0f 61 d7             punpcklwd 61 cf             punpcklwd %xmm7,%xmm1
 3ea:   66 0f 76 d4             pcmpeqd %xmm4,%xmm2
 3ee:   66 0f 76 e1             pcmpeqd %xmm1,%xmm4
 3f2:   66 0f db d3             pand   %xmm3,%xmm2
 3f6:   66 0f fe fa             paddd  %xmm2,%xmm7
 3fa:   66 0f db e3             pand   %xmm3,%xmm4
 3fe:   66 0f fe fc             paddd  %xmm4,%xmm7
 402:   66 0f 6f ef             movdqa %xmm7,%xmm5
 406:   66 0f 73 dd 08          psrldq $0x8,%xmm5
 40b:   66 0f fe fd             paddd  %xmm5,%xmm7
 40f:   66 0f 6f f7             movdqa %xmm7,%xmm6
 413:   66 0f 73 d6 20          psrlq  $0x20,%xmm6
 418:   66 0f fe fe             paddd  %xmm6,%xmm7
 41c:   66 0f 7e fa             movd   %xmm7,%edx
 420:   ff ca                   dec    %edx


what should happen is we splay the match value into one 8 reg
the "to be matched" into another...
the and into another.
then you mask out 0x1 in each for the comparsons
Then you do a horizontal add of everything
and bring it back into the main register.


Maybe that's sse3 only and I didn't enable it?

_mm_hadd_epi16

_mm_hadds_pi16

** Guided Auto Parallelization

The Intel compiler includes a Guided Auto Parallelization (GAP) feature that can help analyze source code and generate advice on how to obtain better performance. In particular, GAP will suggest code changes or compiler options that will lead to better vectorized code. GAP may optionally allow the user to take advantage of the auto-parallelization capability that can generate multithreaded code for independent loop iterations; however, developers are encouraged to use explicit thread parallelism through mechanisms like OpenMP.

The GAP feature can be accessed by adding the -guide option, which takes an optional =# parameter, where # can be a number between 1 and 4 with 1 being the lowest level of guidance and 4 (the default) being the most advanced level of guidance.  The compiler will print a GAP report to stderr or it can be redirected to a file with the -guide-file=filename option, which will send the output to the file name filename, or the -guide-file-append=filename option, which will append to the specified file.  The GAP analysis can be targeted to a specific file, function, or source line with the -guide-opt=specification option. Refer to the compiler man pages or documentation for details on this option.

https://computing.llnl.gov/?set=code&page=intel_vector

**  3c6:   00 
 3c7:   41 0f b7 7d 00          movzwl 0x0(%r13),%edi
 3cc:   f3 41 0f 7e 14 24       movq   (%r12),%xmm2
 3d2:   f3 41 0f 7e 4c 24 08    movq   0x8(%r12),%xmm1
 3d9:   66 0f 6e c7             movd   %edi,%xmm0
 3dd:   66 0f 70 e0 00          pshufd $0x0,%xmm0,%xmm4
 3e2:   66 0f 61 d7             punpcklwd %xmm7,%xmm2
 3e6:   66 0f 61 cf             punpcklwd %xmm7,%xmm1
 3ea:   66 0f 76 d4             pcmpeqd %xmm4,%xmm2
 3ee:   66 0f 76 e1             pcmpeqd %xmm1,%xmm4
 3f2:   66 0f db d3             pand   %xmm3,%xmm2
 3f6:   66 0f fe fa             paddd  %xmm2,%xmm7
 3fa:   66 0f db e3             pand   %xmm3,%xmm4
 3fe:   66 0f fe fc             paddd  %xmm4,%xmm7
 402:   66 0f 6f ef             movdqa %xmm7,%xmm5
 406:   66 0f 73 dd 08          psrldqpsrldq $0x8,%xmm5
 40b:   66 0f fe fd             paddd  %xmm5,%xmm7
 40f:   66 0f 6f f7             movdqa %xmm7,%xmm6
 413:   66 0f 73 d6 20          psrlq  $0x20,%xmm6
 418:   66 0f fe fe             paddd  %xmm6,%xmm7
 41c:   66 0f 7e fa             movd   %xmm7,%edx
 420:   ff ca                   dec    %edx
 422:   41 0f b7 34 24          movzwl (%r12),%esi

**  It seems likely that I am still smarter than the compiler.

Except that I'm not able to outsmart it. I can do the damn job in
*2* registers. Not 7. And in 11ins. Admitted phadd is 
pretty slow, but...

movq match reg1
mov 01 into reg2
(can't I just put them all in at the same time)?
splay match (might stash it somewhere in a more optimized version
splay 01 reg2
PCMPEQW reg1, tbl b* # destroying reg1
pandw reg2, reg1 # destroying reg2

* horizontal add reg2, reg2 # with itself:
PHADDW xmm1, xmm1 gives me 4 arrays of 16
PHADD xmm1, xmm1 gives me 2 vecs arrays of 32
shuffle
phadd xmm1 xmm1
grab the result

I can even do this 256 bytes wide now with avx.

there's probably another way to do this.

* I can get away from the mask and do a lookup
in cases where I will have 1 and only 1 match

movq match reg1
splay reg1
stash reg3 reg1 ; keep a copy
loop: PCMPEQW reg1, tbl b* # destroying reg1
padd4 ; darn, can overflow if I have more matches - but I wont in most cases
this is used
; 64 bits I got now adays!
shuffle the two vectors into 64 bits
mov to ecx
** 6 sse ins!
; lookup the result - if it has any ones we had a hit and have to backtrack
test
beq have_hit
; otherwise we motor on...
inc tablb b + 16; can get interleaved elsewhere (after the padd)
mov reg1 reg3; can get interleaved elsewhere (with the padd)
goto loop:

have_hit:
; if I interleave, we need to backtrack 32, not 16
to turn the result in a lookuptable... - let's see...

* mov fix

shufle reg1, match 

* aarch Neon version
vld  reg2 *tbl a; load all lanes with lowest value
vld; reg0 copy the lowest 16 bits
loop: vld; reg1 tblb
cmp: reg1, reg1, reg0
padd4 reg1, reg1, reg1 - padd the result - again: can overflow, bad
shufl - get result into one reg? (maybe can skip)
store to main unit
test
beq have_hit
vld reg1 tblb preindexed
goto loop: 

* Overflow is scary

I think the and version is just safer. overflows make me nervous,
and we get a value from 0,1,2,4, | the same shifted << 32

so the backtracker just has to pull the high value from the
unit if there's a miss in the early dword.

* And gcc doesn't like the pragma

BUT does successfully vectorize and use a load and only 3 regs.

Yea!

 30d:   66 0f 61 c9             punpcklwd %xmm1,%xmm1
 311:   66 0f 70 c9 00          pshufd $0x0,%xmm1,%xmm1
 316:   66 0f 75 0e             pcmpeqw (%rsi),%xmm1
 31a:   66 0f db 0d 00 00 00    pand   0x0(%rip),%xmm1        # 322 <tbl_a_tbl_b_roar_match_firsthit_vvvector+0x32>
 321:   00 
 322:   66 0f 6f c1             movdqa %xmm1,%xmm0
 326:   66 0f 61 ca             punpcklwd %xmm2,%xmm1
 32a:   66 0f 69 c2             punpckhwd %xmm2,%xmm0
 32e:   66 0f fe c8             paddd  %xmm0,%xmm1
 332:   66 0f 6f d1             movdqa %xmm1,%xmm2
 336:   66 0f 6f c1             movdqa %xmm1,%xmm0
 33a:   66 0f 73 da 08          psrldq $0x8,%xmm2
 33f:   66 0f fe c2             paddd  %xmm2,%xmm0
 343:   66 0f 6f c8             movdqa %xmm0,%xmm1
 347:   66 0f 73 d9 04          psrldq $0x4,%xmm1
 34c:   66 0f fe c1             paddd  %xmm1,%xmm0
 350:   66 0f 7e c2             movd   %xmm0,%edx
 354:   83 ea 01                sub    $0x1,%edx
 357:   eb 0b                   jmp    364 <tbl_a_tbl_b_roar_match_firsthit_vvvector+0x74>

* Future Notes for auto vectorization

Loops should not be pointers, but array indexes.
There needs to be a clear "for" component with a fixed
termination.

It is totally feasible to use my & and freerunning tricks but the compiler
doesn't like it.

* In an orgy of excess

still not there... 

search 255 entries in a single go, not vectored

 0f     b6 c8                   movzbl %al,%ecx
 1f8    66 39 7c 4b fe          cmp    %di,-0x2(%rbx,%rcx,2)
 200:   0f 94 c1                sete   %cl
 203:   01 ca                   add    %ecx,%edx
 205:   04 01                   add    $0x1,%al
 207:   75 ef                   jne    1f8 <tbl_a_tbl_b_roar_match_freerun_vvectormaybe+0x18>
 209:   0f b7 73 fe             movzwl -0x2(%rbx),%esi
 20d:   0f b6 d2                movzbl %dl,%edx
 210:   e8 00 00 00 00          callq  215 <tbl_a_tbl_b_roar_match_freerun_vvectormaybe+0x35>


SOMETIMES_INLINE tbl_b PO(roar_match_freerun_vvectormaybe)(tbl_a* a, tbl_b* B_ALIGNED b)
{
  b = __builtin_assume_aligned (b, 16);
  unsigned char d;
  unsigned char r = -1;
  tbl_a match = *a;
  b--;
  #pragma simd
  for(d = 1; d != 0; d++) // I love writing infinite looking loops even tho icc will complain
	  r += (match == b[d]);
  r = RESULT(r, a, b);

  return *b;
}


// like so much here, this is not correct code
SOMETIMES_INLINE tbl_b PO(roar_match_freerun_vvectormaybe)(tbl_a* a, tbl_b* B_ALIGNED b)
{
  b = __builtin_assume_aligned (b, 16);
  unsigned char d;
  unsigned char r = -1;
  tbl_a match = *a;
  b--;
  #pragma simd
  for(d = 1; d ; d++) // I love writing infinite looking loops even tho icc will complain
	  r += (match == b[d]);
  r = RESULT(r, a, b);

  return *b;
}


00000000000001e0 <tbl_a_tbl_b_roar_match_freerun_vvectormaybe>:
 1e0:   53                      push   %rbx
 1e1:   0f b7 3f                movzwl (%rdi),%edi
 1e4:   48 89 f3                mov    %rsi,%rbx
 1e7:   ba ff ff ff ff          mov    $0xffffffff,%edx
 1ec:   b8 01 00 00 00          mov    $0x1,%eax
 1f1:   0f 1f 80 00 00 00 00    nopl   0x0(%rax)
 1f8:   0f b6 c8                movzbl %al,%ecx
 1fb:   66 39 7c 4b fe          cmp    %di,-0x2(%rbx,%rcx,2)
 200:   0f 94 c1                sete   %cl
 203:   01 ca                   add    %ecx,%edx
 205:   04 01                   add    $0x1,%al
 207:   75 ef                   jne    1f8 <tbl_a_tbl_b_roar_match_freerun_vvectormaybe+0x18>
 209:   0f b7 73 fe             movzwl -0x2(%rbx),%esi
 20d:   0f b6 d2                movzbl %dl,%edx
 210:   e8 00 00 00 00          callq  215 <tbl_a_tbl_b_roar_match_freerun_vvectormaybe+0x35>
 215:   0f b7 43 fe             movzwl -0x2(%rbx),%eax
 219:   5b                      pop    %rbx
 21a:   c3                      retq   
 21b:   0f 1f 44 00 00          nopl   0x0(%rax,%rax,1)

** But only this will vectorize
// like so much here, this is not correct code
SOMETIMES_INLINE tbl_b PO(roar_match_freerun_vvectormaybe2)(tbl_a* a, tbl_b* B_ALIGNED b)
{
  b = __builtin_assume_aligned (b, 16);
  unsigned char d;
  unsigned char r = -1;
  tbl_a match = *a;
  #pragma simd
  for(d = 0; d < 255 ; d++) // I love writing infinite looking loops even tho icc will complain
	  r += (match == b[d]);
  r = RESULT(r, a, b);

  return *b;
}

which it does a pretty huge complete unroll on which
is probably not what I want either

// Still I like constraining my space this way:

0000000000000220 <tbl_a_tbl_b_roar_match_freerun_vvectorno_firstexit>:
 220:   53                      push   %rbx
 221:   0f b7 3f                movzwl (%rdi),%edi
 224:   48 89 f3                mov    %rsi,%rbx
 227:   b8 01 00 00 00          mov    $0x1,%eax
 22c:   0f 1f 40 00             nopl   0x0(%rax)
 230:   0f b6 d0                movzbl %al,%edx
 233:   66 39 7c 53 fe          cmp    %di,-0x2(%rbx,%rdx,2)
 238:   0f 94 c2                sete   %dl
 23b:   83 c0 01                add    $0x1,%eax
 23e:   83 ea 01                sub    $0x1,%edx
 241:   84 c2                   test   %al,%dl
 243:   75 eb                   jne    230 <tbl_a_tbl_b_roar_match_freerun_vvectorno_firstexit+0x10>
 245:   0f b7 73 fe             movzwl -0x2(%rbx),%esi
 249:   0f b6 d2                movzbl %dl,%edx
 24c:   e8 00 00 00 00          callq  251 <tbl_a_tbl_b_roar_match_freerun_vvectorno_firstexit+0x31>
 251:   0f b7 43 fe             movzwl -0x2(%rbx),%eax
 255:   5b                      pop    %rbx
 256:   c3                      retq   
 257:   66 0f 1f 84 00 00 00    nopw   0x0(%rax,%rax,1)

// like so much here, this is not correct code
SOMETIMES_INLINE tbl_b PO(roar_match_freerun_vvectorno_firstexit)(tbl_a* a, tbl_b* B_ALIGNED b)
{
  b = __builtin_assume_aligned (b, 16);
  unsigned char d;
  unsigned char r = -1;
  tbl_a match = *a;
  b--;
  #pragma simd
  for(d = 1; d & r; d++) // I love writing infinite looking loops even tho icc will complain
	  r += (match == b[d]);
  r = RESULT(r, a, b);

  return *b;
}

and if I need a shorter search, constraining d = 256 - 16


In file included from tables_search.c(22):
../includes/tables_search.h(138): error: parallel loop condition operator must be one of <, <=, >, >=, or !=
    for(d = 1; d ; d++) // I love writing infinite looking loops even tho icc will complain

* And moving to neon

except that an infinit loop crept into all the rest of the code.

used to have "do":

 while(r = RESULT(r, a, b)) ;

anyway, finally generating code for neon, too. Still about twice as much as I expected

00000298 <tbl_a_tbl_b_roar_match_firsthit_vvector>:
 298:   8802            ldrh    r2, [r0, #0]
 29a:   efc0 2811       vmov.i16        d18, #1 ; 0x0001
 29e:   f961 17df       vld1.64 {d17}, [r1 :64]
 2a2:   efc0 0010       vmov.i32        d16, #0 ; 0x00000000
 2a6:   b538            push    {r3, r4, r5, lr}
 2a8:   ee83 2bb0       vdup.16 d19, r2
 2ac:   460c            mov     r4, r1
 2ae:   8909            ldrh    r1, [r1, #8]
 2b0:   ff51 18b3       vceq.i16        d17, d17, d19
 2b4:   4605            mov     r5, r0
 2b6:   4610            mov     r0, r2
 2b8:   ff62 01b1       vbit    d16, d18, d17
 2bc:   ffd0 0a30       vmovl.u16       q8, d16
 2c0:   ef61 08a0       vadd.i32        d16, d17, d16
 2c4:   ef60 0bb0       vpadd.i32       d16, d16, d16
 2c8:   ee10 3b90       vmov.32 r3, d16[0]
 2cc:   4291            cmp     r1, r2
 2ce:   bf0c            ite     eq
 2d0:   461a            moveq   r2, r3
 2d2:   1e5a            subne   r2, r3, #1
 2d4:   e000            b.n     2d8 <tbl_a_tbl_b_roar_match_firsthit_vvector+0x40>
 2d6:   8828            ldrh    r0, [r5, #0]
 2d8:   8821            ldrh    r1, [r4, #0]
 2da:   f7ff fffe       bl      0 <my_result>
 2de:   4602            mov     r2, r0
 2e0:   2800            cmp     r0, #0
 2e2:   d1f8            bne.n   2d6 <tbl_a_tbl_b_roar_match_firsthit_vvector+0x3e>
 2e4:   8820            ldrh    r0, [r4, #0]
 2e6:   bd38            pop     {r3, r4, r5, pc}

* TODO Big stuff <2017-03-20 Mon> 
** Accomplishments
*** made a dent in structure passing (see exper)
structure return remains problematic. - need to look at that harder
what I did last time was just violate the API.
*** Finally vectorized stuff
and hopefully with serious inlining I won't have to think about it much...
BUT I think I can still write 2-3x better code than what I'm seeing
for the core inner loops.
*** Got a clean API emerging for erm
If I wasn't so hung up on efficient structure passing
* Next up:
** Decide on time format and API.
I'm leaning back to nanoseconds since the build date + hour
** Decide on librcu trial or bsp
BSP has apparent simplicity and runs on the parallella already
RCU is smaller and tighter
** Give shared mem on the vfs another go
** Work on virtual fs
** Think about a doxygen-like thing
** Improve my emacs more
*** (timestamps time logging etc - and fkeys!)
*** win the war with org
*** win the war with clang
** Rework headers and further abstract makefile
** Oh yea, make compile again.
** Resolved: write one tedious function a day. In the morning.
I might stop checking in my nightly messes. But I do like seeing the
before/after even if nobody else does.
*** Bit endian converter for ips
Just a generic one. Can do SSE and NEON later no matter how 
tempted I am.
*** Actual working table lookups
*** Replace the std apis for:
**** interfaces
**** addrinfo
**** printing ipv6 addresses (And scanning them)
**** and whatever else in the c lib I don't need
**** printf hooks
tho I want to kill printf entirely
*** Declarative Virtual Machine start, reg placement and setup api
*** DONE gdb hooks for bits
print /t $xmm1.uint128
can't remember how to throw them in a frame


** Try to find a decent packet parsing language
I really want something abstract. P4? What?
VPP?

* Doxygen. I personally find this sort of doc more annoying than useful

Then again, I'm not liking my opinionated // comments that much either.

And I already have way more comments than code. And what is the point of
repeating the @param and @return from the code itself?

but what does the second @code statement mean?

qlib looked interesting.

/**
 * Get 32-bit Murmur3 hash.
 *
 * @param data      source data
 * @param nbytes    size of data
 *
 * @return 32-bit unsigned hash value.
 *
 * @code
 *  uint32_t hashval = qhashmurmur3_32((void*)"hello", 5);
 * @endcode
 *
 * @code
 *  MurmurHash3 was created by Austin Appleby  in 2008. The initial
 *  implementation was published in C++ and placed in the public.
 *    https://sites.google.com/site/murmurhash/
 *  Seungyoung Kim has ported its implementation into C language
 *  in 2012 and published it as a part of qLibc component.
 * @endcode
 */

* Riscv compiler version

** TODO Gotta get this thing re-loaded into the fpga

* Back to my snark (blanusa)

We have to solve the same problem at least 3 different ways for an optimal set
of solutions, and choose three.

1) The *best* route - that is the one currently shown by most of the parameters
   to be the one with

- The highest bandwidth
- the lowest latency
- the least packet loss
- The shortest number of hops to the destination
- the highest available bandwidth to that destination

(BGP has something like 14 possible goals in total, including financial cost)

2) The second best route has the following properties
- Not the same device as the best route (in case that fails)
- the best set of the above properties

3) The third best route has a different set of properties than the first two.

And I don't need any floating point. At least, not for REALLY small values.
Several of these are pure functions but to scribble them down:

The second one I'm too tired to fill in the margins of this notebook.

The simpler derivatives, also.

* Now that I have my potential licensing problems sorted

I dared look at bird. Yep, it's been solving all the same problems (and in
nearly every case, *really well*) for decades now. Many similar abstractions to
what I've been putting in exist, and the code is really tight. Even the codebase
style is very similar to my native one. The "slab" concept is quite similar to
my generational table lookup idea. (I know slabs are nothing new). It uses unix
domain sockets to communicate.

So good, I can work on bird-2.0 babel and get a version of that that compiles
down small (and has ipv4 support)and maybe wedge in a few other features.

That said there are a few major differences between what I'm trying to do here
and how bird does it. I started writing this table only to have emacs break.

cond: Symbol's function definition is void: org-table-justify-field-maybe

|Item|Bird |Babeld|Erm|Note|
|timebase|seconds|usec|nanosec|Fixable|
|ip comparisons|xor|memcmp|xor|joy|
|hashes|yes|no|yes|joy|
|popcount|no|no|yes|hook is in there|
|fs|no|no|yes|doable|
|protocols|all|babel|none|sigh|
|indexes|some|none|all|

* YAML has promise

here I am fiddling with yacc and peg, when the world has moved onto yaml.

* And I finally dared look, also, at vpp

It's pretty portable. An OS all by itself. It's *ginormous*.
Merely watching it build caused me to lose all the motivation
I've had for weeks.

I've enjoyed myself enormously writing this thing, but I guess
It's time to go and see what vpp and ddpk can do for me, instead.

I took a walk. It's still building.
$

* Back to vector processing

vdup takes conditionals, so you can more easily load up your choice of mask

VDUP{cond}.size Qd, Dm[x]
VDUP{cond}.size Dd, Dm[x]
VDUP{cond}.size Qd, Rm


size
must be 8, 16, or 32.
Qd

everything takes conditionals

** IMPORTANT: xVHADD is not a horizontal ADD (I thought it was). It is a halving add.

VHADD adds corresponding elements in two vectors, shifts each result right one bit, and places the results in the destination vector. Results are truncated.


http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0473m/dom1361289949173.html

VLDM can load up to 8 q registers at a time.

VMAX compares corresponding elements in two vectors, and copies the larger of each pair into the corresponding element in the destination vector.

VMOV does let me use immediate (floating point format) constants


VMOVN copies the least significant half of each element of a quadword vector into the corresponding elements of a doubleword vector.


VMOV2 can generate any 16-bit immediate value, and a restricted range of 32-bit and 64-bit immediate values.

The VMRS instruction transfers the contents of extsysreg into Rd.

stalls until all other neon ins are complete

VMVN inverts the value of each bit from the source register and places the
results into the destination register

VMVN inverts the value of each bit from an immediate value and places the results into each element in the destination register.


VQABS takes the absolute value of each element in a vector, and places the results in a second vector.
The sticky QC flag (FPSCR bit[27]) is set if saturation occurs.

VQADD adds corresponding elements in two vectors, and places the results in the destination vector.
The sticky QC flag (FPSCR bit[27]) is set if saturation occurs.

elsewhere I was ranting about the wastebin idea. with neon condition codes I
don't need that wastebin

12.118 VSTn (multiple n-element structures)
Vector Store multiple n-element structures.

VUZP might be a start at doing the bit reversal stuff on two IP addrs at the same
time.

VCLZ counts the number of consecutive zeros, starting from the top bit, in each element in a vector, and places the results in a second vector.

* ebusl - ebuse musl

* Sigh. More assembly

#define syscall4( number, _1, _2, _3, _4 )  \
({                                          \
    int64_t ret;                            \
    register int64_t r10 asm("r10") = _4;   \
    __asm__ volatile                        \
    (                                       \
        "syscall\n\t"                       \
        : "=a"( ret )                       \
        : "a"( number ),                    \
          "D"( _1 ),                        \
          "S"( _2 ),                        \
          "d"( _3 ),                        \
          "r"( r10 )                        \
        : "memory", "rcx", "r11"            \
    );                                      \
    ret;                                    \
})

* Compare
** My original popcounter came to 97 bytes and could only caculate one popcount


  4006c9:       53                      push   %rbx
  4006ca:       31 db                   xor    %ebx,%ebx
  4006cc:       48 8b 3d e5 1a 20 00    mov    0x201ae5(%rip),%rdi        # 6021b8 <test1+0x18>
  4006d3:       48 89 d8                mov    %rbx,%rax
  4006d6:       48 8b 35 d3 1a 20 00    mov    0x201ad3(%rip),%rsi        # 6021b0 <test1+0x10>
  4006dd:       48 8b 0d c4 1a 20 00    mov    0x201ac4(%rip),%rcx        # 6021a8 <test1+0x8>
  4006e4:       48 8b 15 b5 1a 20 00    mov    0x201ab5(%rip),%rdx        # 6021a0 <test1>
  4006eb:       49 89 d9                mov    %rbx,%r9
  4006ee:       49 89 da                mov    %rbx,%r10
  4006f1:       49 89 db                mov    %rbx,%r11
  4006f4:       f3 48 0f b8 d2          popcnt %rdx,%rdx
  4006f9:       48 01 d0                add    %rdx,%rax
  4006fc:       f3 48 0f b8 c9          popcnt %rcx,%rcx
  400701:       49 01 c9                add    %rcx,%r9
  400704:       f3 48 0f b8 f6          popcnt %rsi,%rsi
  400709:       49 01 f2                add    %rsi,%r10
  40070c:       f3 48 0f b8 ff          popcnt %rdi,%rdi
  400711:       49 01 fb                add    %rdi,%r11
  400714:       4c 89 d9                mov    %r11,%rcx
  400717:       4c 89 ca                mov    %r9,%rdx
  40071a:       be b4 0b 40 00          mov    $0x400bb4,%esi
  40071f:       4c 01 d1                add    %r10,%rcx
  400722:       48 01 c2                add    %rax,%rdx
  400725:       bf 01 00 00 00          mov    $0x1,%edi
  40072a:       31 c0                   xor    %eax,%eax

** The "final one" takes up 68 bytes and can do 1-8

And the only reason why it takes up that many is because of the printf got
more complex to handle up to 8 return values.

From start to finish it is essentially 39 bytes (vs 72 or more, depending on how
you count, but the inline version scribbled on a ton of regs that had be
restored).

  4009b7:       b9 08 00 00 00          mov    $0x8,%ecx
  4009bc:       48 8d 05 bd 16 20 00    lea    0x2016bd(%rip),%rax        # 6020
80 <test6>

00000000004009c3 <popcnt_again366>:
  4009c3:       48 c1 e2 08             shl    $0x8,%rdx
  4009c7:       f3 48 0f b8 30          popcnt (%rax),%rsi
  4009cc:       48 01 f2                add    %rsi,%rdx
  4009cf:       f3 48 0f b8 70 08       popcnt 0x8(%rax),%rsi
  4009d5:       48 83 c0 10             add    $0x10,%rax
  4009d9:       48 01 f2                add    %rsi,%rdx
  4009dc:       e2 e5                   loop   4009c3 <popcnt_again366>
  4009de:       0f b6 ca                movzbl %dl,%ecx
  4009e1:       66 c1 fa 08             sar    $0x8,%dx
  4009e5:       48 89 05 94 16 20 00    mov    %rax,0x201694(%rip)        # 602080 <test6>
  4009ec:       0f bf d2                movswl %dx,%edx
  4009ef:       be 02 0c 40 00          mov    $0x400c02,%esi
  4009f4:       bf 01 00 00 00          mov    $0x1,%edi
  4009f9:       31 c0                   xor    %eax,%eax
  4009fb:       e8 50 fb ff ff          callq  400550 <__printf_chk@plt>

* And... the compiler wins!

The compiler version is 40 bytes.

00000000004005a0 <popcount2generic>:
  4005a0:       31 d2                   xor    %edx,%edx
  4005a2:       31 c0                   xor    %eax,%eax
  4005a4:       39 f2                   cmp    %esi,%edx
  4005a6:       7d 1d                   jge    4005c5 <popcount2generic+0x25>
  4005a8:       f3 48 0f b8 0f          popcnt (%rdi),%rcx
  4005ad:       48 83 c7 10             add    $0x10,%rdi
  4005b1:       48 c1 e0 08             shl    $0x8,%rax
  4005b5:       f3 4c 0f b8 47 f8       popcnt -0x8(%rdi),%r8
  4005bb:       ff c2                   inc    %edx
  4005bd:       4c 01 c1                add    %r8,%rcx
  4005c0:       48 01 c8                add    %rcx,%rax
  4005c3:       eb df                   jmp    4005a4 <popcount2generic+0x4>
  4005c5:       c3                      retq   

vs mine (36 bytes). (and the two results differed because we go *down*, AND the
compiler version checks for 0 on entry. Sigh.

0000000000400c40 <popcount2anydest>:
  400c40:       89 f1                   mov    %esi,%ecx
  400c42:       48 8d 17                lea    (%rdi),%rdx

0000000000400c45 <popcnt_again133>:
  400c45:       48 c1 e0 08             shl    $0x8,%rax
  400c49:       f3 48 0f b8 32          popcnt (%rdx),%rsi
  400c4e:       48 01 f0                add    %rsi,%rax
  400c51:       f3 48 0f b8 72 08       popcnt 0x8(%rdx),%rsi
  400c57:       48 83 c2 10             add    $0x10,%rdx
  400c5b:       48 01 f0                add    %rsi,%rax
  400c5e:       e2 e5                   loop   400c45 <popcnt_again133>
  400c60:       48 89 17                mov    %rdx,(%rdi)
  400c63:       c3                      retq   
  400c64:       66 2e 0f 1f 84 00 00    nopw   %cs:0x0(%rax,%rax,1)
  400c6b:       00 00 00 
  400c6e:       66 90                   xchg   %ax,%ax

** And mine is still wrong even after fixing the sense of the 
direction and initializing the total (And coming to the same number of bytes
now. :())

0000000000400c60 <popcount2anydest>:
  400c60:       89 f1                   mov    %esi,%ecx
  400c62:       31 c0                   xor    %eax,%eax
  400c64:       48 8d 17                lea    (%rdi),%rdx

0000000000400c67 <popcnt_again135>:
  400c67:       48 c1 e0 08             shl    $0x8,%rax
  400c6b:       f3 48 0f b8 32          popcnt (%rdx),%rsi
  400c70:       48 01 f0                add    %rsi,%rax
  400c73:       f3 48 0f b8 72 f8       popcnt -0x8(%rdx),%rsi
  400c79:       48 83 c2 10             add    $0x10,%rdx
  400c7d:       48 01 f0                add    %rsi,%rax
  400c80:       e2 e5                   loop   400c67 <popcnt_again135>
  400c82:       48 89 17                mov    %rdx,(%rdi)
  400c85:       c3                      retq   

(unless the other one is wrong)

There is much to learn here, but I think I'll go with the generic version til I
"get leveraging the rdi properly.

* Maybe a hand written version still lacking pre-amble?

I don't know why the compiler uses %r8? but I guess that makes for better out of
order behavior and in a subroutine, it's known to be scratch?

I do begrudge the extra cmp in how I wrote the generic version, and the
unconditional jump the compiler puts in (2 branches vs 1 on entry)

0000000000400c60 <popcount2anydest>:
  400c60:       89 f1                   mov    %esi,%ecx
  400c62:       31 c0                   xor    %eax,%eax

0000000000400c67 <popcnt_again135>:
  400c67:       48 c1 e0 08             shl    $0x8,%rax
  400c6b:       f3 48 0f b8 32          popcnt (%rdi),%r8
  400c70:       48 01 f0                add    %r8,%rax
  400c73:       f3 48 0f b8 72 f8       popcnt -0x8(%rdi),%r8
  400c79:       48 83 c2 10             add    $0x10,%rdi
  400c7d:       48 01 f0                add    %r8,%rax
  400c80:       e2 e5                   loop   400c67 <popcnt_again135>
  400c85:       c3                      retq

And this started off as an exercise in getting post-increment addressing modes
working on another arch entirely, too.

* We don't need no steenking jhashes.

Xors may well suffice. It's a set associative lookup cache, after all.

Because worse is better.

* try using Ripgrep

in emacs

* Did I just hear %r12 was free for C?

One thing I did years ago was a objdump map of the whole library to see what the
register usage actually was.

* Wastebin

1MB of memory mapped on itself on the page size. Write only, also.

https://en.wikipedia.org/wiki/Write-only_memory_(joke)

* if only these were wider

http://www.greenarraychips.com/home/documents/greg/DB002-110705-G144A12.pdf

* Parallella simulator is WAY easier

sudo mkdir -p /opt/adapteva
cd /opt/adapteva
tar xf esdk.2016.11.x86_64.tar.gz
sudo ln -s esdk.2016.11 esdk

3.

Append to ~/.bashrc:

EPIPHANY_HOME=/opt/adapteva/esdk
. ${EPIPHANY_HOME}/setup.sh


https://github.com/adapteva/epiphany-sdk/releases/tag/esdk-2016.11

$ e-sim --preset parallella16 --host ./run.sh

* SSE equality

Might as well stick with sse4.2

Via:
http://stackoverflow.com/questions/26880863/sse-testing-equality-between-two-m128i-variables

3 ins:
if(_mm_test_all_ones(_mm_cmpeq_epi8(v1,v2))) {
    //v0 == v1
}

two ins, setting 
__m128i neq = _mm_xor_si128(v1,v2);
if(_mm_test_all_zeros(neq,neq)) {
    //v0 == v1
}

pxor    %xmm1, %xmm0
ptest   %xmm0, %xmm0

AVX512 has a cmpneq instruction

if (_mm_testz_si128(vcmp, vcmp))             // PTEST (requires SSE 4.1)
{
    // v0 == v1
}


This is similar to the TEST instruction, in that it sets the Z flag to the result of an AND between its operands: ZF is set, if DEST AND SRC is equal to 0. Additionally it sets the C flag if (NOT DEST) AND SRC equals zero.
This is equivalent to setting the Z flag if none of the bits masked by SRC are
set, and the C flag if all of the bits masked by SRC are set.

PHMINPOSUW
Sets the bottom unsigned 16-bit word of the destination to the smallest unsigned
16-bit word in the source, and the next-from-bottom to the index of that word in
the source.

	
@harold I thought _mm_minpos_epu16 was MS specific. I will try that out. – Alexandros Feb 18 '15 at 4:16
1	 	
XOR with _mm_set1_epi16(0x8000) if you want to find the signed minimum – harold Feb 18 '15 at 7:32


x <s y = (x ^ signbit) <u (y ^ signbit)
x <u y = (x ^ signbit) <s (y ^ signbit)
This property transfers directly to min and max, so:

min_s(x, y) = min_u(x ^ signbit, y ^ signbit) ^ signbit
And then we can use _mm_minpos_epu16 to handle the horizontal minimum, to get, in total, something like

__m128i xs = _mm_xor_si128(x, _mm_set1_epi16(0x8000));
return _mm_extract_epi16(_mm_minpos_epu16(xs), 0) - 0x8000;

* UTF enhancements

https://en.wikipedia.org/wiki/Mathematical_operators_and_symbols_in_Unicode

https://www.utf8icons.com/character/8817/neither-greater-than-nor-equal-to

≠
≟
≡ identical to
≢ 
≣ Strictly equivalent to
≤ 
≥
≦
≧
≨ less than but not equal to
≩
≪ much less than

≬ - between

≭ Not equivalnt to
≲

≶ less or greater than

≺ precedes
≻ succeeds

⊀ does not precede

⊂ subset of
⊃

⊎ multiset union

⊕
⊖
⊗
⊚

𝜶

-fextended-identifiers and pre-process your code to convert the identifiers to
 UCN. From the linked page:

perl -pe 'BEGIN { binmode STDIN, ":utf8"; } s/(.)/ord($1) < 128 ? $1 : sprintf("\\U%08x", ord($1))/ge;' 

* cake

In the home environment - only the bottommost 8 bits
are used in the src address.

 the dest address - high order 24.

https://github.com/aappleby/smhasher


⧝

TIE OVER INFINITY (HTML &#10717;)

__m128i xs = _mm_xor_si128(x, _mm_set1_epi16(0x8000));
return _mm_extract_epi16(_mm_minpos_epu16(xs), 0) - 0x8000;


http://www.agner.org/optimize/optimizing_assembly.pdf
-finput-charset=UTF-8 -fextended-identifiers


 The source operand comes before the destination operand.
Register names have %% prefix, which is changed to % before the string is passed on to the
assembler. Constants have $ prefix. Memory operands are also different. For example,
[ebx+ecx*4+20h] is changed to 0x20(%%ebx,%%ecx,4).


The following example finds the minimum of two unsigned numbers: if (b > a) b = a;
; Example 9.17a, Find minimum of eax and ebx (unsigned):
sub eax, ebx ; = a-b
sbb edx, edx ; = (b > a) ? 0xFFFFFFFF : 0
and edx, eax ; = (b > a) ? a-b : 0
add ebx, edx ; Result is in ebx
