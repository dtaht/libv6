* Another crazy idea

Instead of holding the protocol constant in the kernel, use one per routerid and flip their metrics around to suit.

Or use tables to do this. The kernel flushes a given route from a table for us, the other takes over.

Think about this.


root@dancer:~# ip -6 route add fc87::/64 dev eno1 table 902 proto 45 metric 1025 
root@dancer:~# ip -6 route add fc87::/64 dev eno1 table 903 proto 45 metric 1025 
root@dancer:~# ip -6 route show proto 45 (or flush)
doesn't work, we have to know the table

We can flush with a single proto patch

RTNETLINK answers: File exists
root@dancer:~# ip -6 route add fc80::/64 dev eno1 proto 46 metric 1025
root@dancer:~# ip -6 route add fc80::/64 dev eno1 proto 47 metric 1026
root@dancer:~# ip -6 route show | grep fc80
fc80::/64 dev eno1 proto 45 metric 1024  pref medium
fc80::/64 dev eno1 proto 46 metric 1025  pref medium
fc80::/64 dev eno1 proto 47 metric 1026  pref medium

Or one per oid? We only allow 20 interfaces currently...
**

root@dancer:~# ip route replace fc66::/64 metric 1025 metric 1027
RTNETLINK answers: No such device
root@dancer:~# ip route replace fc66::/64 dev eno1 metric 1025 metric 1027
root@dancer:~# ip -6 route show fc66
Error: inet prefix is expected rather than "fc66".
root@dancer:~# ip -6 route show | grep fc66
fc66::/64 dev eno1 metric 1024  pref medium
fc66::/64 dev eno1 metric 1025  pref medium
fc66::/64 dev eno1 metric 1026  pref medium
fc66::/64 dev eno1 metric 1027  pref medium
root@dancer:~# ip route change fc66::/64 dev eno1 metric 1027 metric 1028
RTNETLINK answers: No such file or directory
root@dancer:~# ip route change fc66::/64 dev eno1 metric 1025 metric 1028
RTNETLINK answers: No such file or directory


[sudo] password for d: 
root@dancer:~/git/libv6/tests/consistency# cd
root@dancer:~# ip route add fc66::/64 dev eno1
root@dancer:~# ip route add fc66::/64 dev eno1 metric 1025
root@dancer:~# ip route add fc66::/64 dev eno1 metric 1026


* Keep wanting to use the plen for garbage collection

255 or 129 = source specific

or the pointer with a ton of extra bits left over

* Unify metric table?

If we have 5 routers, they might all basically have each roughly
the same metric for a given route, and we don't need to recalculate them all.

So we sort by routerids and merge metrics and then only have to solve for
uncommon metrics between tem.


we can also announce less if we had a way of merging stuff this way


* RCU essentially the metric comparisons

so we can interrupt a calculation in the middle and get a hello out

Can also, then, essentially thread this portion of things.

* Alternative sorting structure
keep one source specific table (by 128 plen ptrs)
keep one regular structure (by 128 plen ptrs)
Use a bitmask to distinguish between which have values and which don't and use
ffs to move between them.

(common values for me are 48, 56, 60, 64, 128 for ipv6, and 16, 20, 22, 24, 32 for ipv4)

* Radix sort rather than quicksort

quicksort works against any length value. Radix works against fixed values.

* Vector saturating subtract (to zero?)

int32x4_t  vqsubq_s32(int32x4_t a, int32x4_t b);   // VQSUB.S32 q0,q0,q0


* BIC

. AND-NOT lets you use the same mask to set bits (with OR) or clear
bits (with AND-NOT). I guess the SSE architects wanted to save an
opcode, and in practice, you’ll find AND-NOT more useful than NOT. BTW
the fastest way to set an XMM register to all ones (0xFFFF….FFFF) is
to compare the register to itself; i.e.:

https://mischasan.wordpress.com/2011/04/04/what-is-sse-good-for-2-bit-vector-operations/


* 

Initialize a vector from a literal bit pattern
These intrinsics create a vector from a literal bit pattern.
int8x8_t    vcreate_s8(uint64_t a);   // VMOV d0,r0,r0

// Set all lanes to the same value (255?)
uint32x4_t  vdupq_n_u32(uint32_t value);

Vector saturating narrow integer

//uint32x2_t vqmovn_u64(uint64x2_t a);  // VQMOVN.U64 d0,q0

uint16x4_t vqmovn_u32(uint32x4_t a);  // VQMOVN.U32 d0,q0


Bitwise Select

* TODO Heh. I was hoist by my own compile guards in the benchmark.

Calling the neon intrinsics twice.

* Some useful looking intrinsics

  //    uint64x2_t tmp = vorr_u32(vget_low_u32(v), vget_high_u32(v));

__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
veorq_s16 (int16x8_t __a, int16x8_t __b)
{
  return __a ^ __b;
}



__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))
vbicq_u64 (uint64x2_t __a, uint64x2_t __b)
{
  return __a & ~__b;
}

__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))
vpmax_f32 (float32x2_t a, float32x2_t b)
{
  float32x2_t result;
  __asm__ ("fmaxp %0.2s, %1.2s, %2.2s"
           : "=w"(result)
           : "w"(a), "w"(b)
           : /* No clobbers */);
  return result;
}

__extension__ static __inline float64_t __attribute__ ((__always_inline__))
vpmaxnmqd_f64 (float64x2_t a)
{
  float64_t result;
  __asm__ ("fmaxnmp %d0,%1.2d"
           : "=w"(result)
           : "w"(a)
           : /* No clobbers */);
  return result;
}

__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
vpmaxq_u32 (uint32x4_t a, uint32x4_t b)
{
  uint32x4_t result;
  __asm__ ("umaxp %0.4s, %1.4s, %2.4s"
           : "=w"(result)
           : "w"(a), "w"(b)
           : /* No clobbers */);
  return result;
}


__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
vpmaxq_u32 (uint32x4_t a, uint32x4_t b)
{
  uint32x4_t result;
  __asm__ ("umaxp %0.4s, %1.4s, %2.4s"
           : "=w"(result)
           : "w"(a), "w"(b)
           : /* No clobbers */);
  return result;
}

Hah. I wondered how they created zeros.

__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))
vqmovn_high_s32 (int16x4_t a, int32x4_t b)
{
  int16x8_t result = vcombine_s16 (a, vcreate_s16 (__AARCH64_UINT64_C (0x0)));
  __asm__ ("sqxtn2 %0.8h, %1.4s"
           : "+w"(result)
           : "w"(b)
           : /* No clobbers */);
  return result;
}

#define vqrshrn_high_n_s64(a, b, c)                                     \
  __extension__                                                         \
    ({                                                                  \
       int64x2_t b_ = (b);                                              \
       int32x2_t a_ = (a);                                              \
       int32x4_t result = vcombine_s32                                  \
                            (a_, vcreate_s32                            \
                                   (__AARCH64_UINT64_C (0x0)));         \
       __asm__ ("sqrshrn2 %0.4s, %1.2d, #%2"                            \
                : "+w"(result)                                          \
                : "w"(b_), "i"(c)                                       \
                : /* No clobbers */);                                   \
       result;                                                          \
     })

#define vset_lane_u8(a, b, c)                                           \
  __extension__                                                         \
    ({                                                                  \
       uint8x8_t b_ = (b);                                              \
       uint8_t a_ = (a);                                                \
       uint8x8_t result;                                                \
       __asm__ ("ins %0.b[%3], %w1"                                     \
                : "=w"(result)                                          \
                : "r"(a_), "0"(b_), "i"(c)                              \
                : /* No clobbers */);                                   \
       result;                                                          \
     })

Does this mean I can store a single byte?

    #define vst1_lane_u8(a, b, c)                                           \
  __extension__                                                         \
    ({                                                                  \
       uint8x8_t b_ = (b);                                              \
       uint8_t * a_ = (a);                                              \
       __asm__ ("st1 {%1.b}[%2],[%0]"                                   \
                :                                                       \
                : "r"(a_), "w"(b_), "i"(c)                              \
                : "memory");                                            \
     })


__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))
vsubhn_high_u32 (uint16x4_t a, uint32x4_t b, uint32x4_t c)

Yes virginia, you cand load 4 lanes
in one shot. There is a union in my future.

__LD2_LANE_FUNC (int64x2x2_t, int64_t, 2d, d, s64, q)

__ST4_LANE_FUNC (uint32x4x4_t, uint32_t, v4si, si, u32)

What's the eq
mean?


vqdmulhq_laneq_s32 (int32x4_t __a, int32x4_t __b, const int __c)
{
  return __builtin_aarch64_sqdmulh_laneqv4si (__a, __b, __c);
}

__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
vqtbl2_u8 (uint8x16x2_t tab, uint8x8_t idx)
{
  uint8x8_t result;
  __asm__ ("ld1 {v16.16b, v17.16b}, %1\n\t"
           "tbl %0.8b, {v16.16b, v17.16b}, %2.8b\n\t"
           :"=w"(result)
           :"Q"(tab),"w"(idx)
           :"memory", "v16", "v17");
  return result;
}


__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
vqtbx1_s8 (int8x8_t r, int8x16_t tab, uint8x8_t idx)
{
  int8x8_t result = r;
  __asm__ ("tbx %0.8b,{%1.16b},%2.8b"
           : "+w"(result)
           : "w"(tab), "w"(idx)
           : /* No clobbers */);
  return result;
}

shift

We already know they are equal to the size
of the mask. A further subtract will yield
postive, negative, or equal (0) to the size of the largest
one

we put in both masks and don't compare
them

can we abuse fp?

__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
vceqq_u8 (uint8x16_t __a, uint8x16_t __b)
{
  return (uint8x16_t) __builtin_aarch64_cmeqv16qi ((int8x16_t) __a,
                                                   (int8x16_t) __b);
}

__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
vceqzq_u8 (uint8x16_t __a)
{
  uint8x16_t __b = {0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0};
  return (uint8x16_t) __builtin_aarch64_cmeqv16qi ((int8x16_t) __a,
                                                   (int8x16_t) __b);
}

__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))
vdup_laneq_u32 (uint32x4_t __a, const int __b)
{
  return __aarch64_vdup_laneq_u32 (__a, __b);
}

__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))
vext_s8 (int8x8_t __a, int8x8_t __b, __const int __c)
{
  __builtin_aarch64_im_lane_boundsi (__c, 8);
#ifdef __AARCH64EB__
  return __builtin_shuffle (__b, __a, (uint8x8_t)
      {8-__c, 9-__c, 10-__c, 11-__c, 12-__c, 13-__c, 14-__c, 15-__c});
#else
  return __builtin_shuffle (__a, __b,
      (uint8x8_t) {__c, __c+1, __c+2, __c+3, __c+4, __c+5, __c+6, __c+7});
#endif
}

It was not clearn this needed four instructions
to express. 

__extension__ static __inline uint64x1x4_t __attribute__ ((__always_inline__))
vld4_u64 (const uint64_t * __a)
{
  uint64x1x4_t ret;
  __builtin_aarch64_simd_xi __o;
  __o = __builtin_aarch64_ld4di ((const __builtin_aarch64_simd_di *) __a);
  ret.val[0] = (uint64x1_t) __builtin_aarch64_get_dregxidi (__o, 0);
  ret.val[1] = (uint64x1_t) __builtin_aarch64_get_dregxidi (__o, 1);
  ret.val[2] = (uint64x1_t) __builtin_aarch64_get_dregxidi (__o, 2);
  ret.val[3] = (uint64x1_t) __builtin_aarch64_get_dregxidi (__o, 3);
  return ret;
}

__extension__ static __inline uint64x2x4_t __attribute__ ((__always_inline__))
vld4q_u64 (const uint64_t * __a)
{ 
  uint64x2x4_t ret;
  __builtin_aarch64_simd_xi __o;
  __o = __builtin_aarch64_ld4v2di ((const __builtin_aarch64_simd_di *) __a);
  ret.val[0] = (uint64x2_t) __builtin_aarch64_get_qregxiv2di (__o, 0);
  ret.val[1] = (uint64x2_t) __builtin_aarch64_get_qregxiv2di (__o, 1);
  ret.val[2] = (uint64x2_t) __builtin_aarch64_get_qregxiv2di (__o, 2);
  ret.val[3] = (uint64x2_t) __builtin_aarch64_get_qregxiv2di (__o, 3);
  return ret;
} 


__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
vmaxv_u32 (uint32x2_t __a)
{
  return vget_lane_u32 ((uint32x2_t)
                __builtin_aarch64_reduc_umax_v2si ((int32x2_t) __a),
                0);
}

This is basically my isnotzer

__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
vmaxvq_u32 (uint32x4_t __a)
{
  return vgetq_lane_u32 ((uint32x4_t)
                __builtin_aarch64_reduc_umax_v4si ((int32x4_t) __a),
                0);
}


__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
vmla_laneq_s32 (int32x2_t __a, int32x2_t __b,
                int32x4_t __c, const int __lane)
{
  return (__a + (__b * __aarch64_vgetq_lane_s32 (__c, __lane)));
}


__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))
vmla_laneq_s32 (int32x2_t __a, int32x2_t __b,
                int32x4_t __c, const int __lane)
{
  return (__a + (__b * __aarch64_vgetq_lane_s32 (__c, __lane)));
}

__extension__ static __inline int16_t __attribute__ ((__always_inline__))
vqmovns_s32 (int32_t __a)
{
  return (int16_t) __builtin_aarch64_sqmovnsi (__a);
}

__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))
vrev16q_p8 (poly8x16_t a)
{
  return __builtin_shuffle (a,
      (uint8x16_t) { 1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14 });
}


vrshl_u8 (uint8x8_t __a, int8x8_t __b)
{
  return __builtin_aarch64_urshlv8qi_uus (__a, __b);
}

static __inline uint32_t
vsha1h_u32 (uint32_t hash_e)
{
  return __builtin_aarch64_crypto_sha1hsi_uu (hash_e);
}

This takes a constant int? but??

__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))
vshlq_n_s64 (int64x2_t __a, const int __b)
{
  return (int64x2_t) __builtin_aarch64_ashlv2di (__a, __b);
}

* Can we sse the cost calc?

    unsigned short refmetric;
    unsigned short cost;
    unsigned short add_metric;
    unsigned short seqno;

We could also break out the metric
table from the main route table
and crunch it seperately

** Aarm 64 bit api
8 params passed forward and back in regs
r9-r15 are scratchr19-r28 are callee saved


te Unlike in AArch32, in AArch64 the 128-bit and 64-bit views of a SIMD and Floating-Point register do not
overlap

simd - v0-v7 are subroutine call/retrun
v8-v15 must be preserved 
v16-v31 must not be preserved

dumb:

Additionally, only the bottom 64-bits of each
value stored in v8-v15 need to be preserved1
The FPSR is a status register that holds the cumulative exception bits of the floating-point unit. It contains the
fields IDC, IXC, UFC, OFC, DZC, IOC and QC. These fields are not preserved across a public interface and 


* Getting ones or zeros is it's own instruction now

MOVI Vn.2D, #uimm64
Move immediate (vector) : replicates a “byte mask immediate” consisting of 8 bytes, each byte having only
the value 0x00 or 0xff, into each 64-bit element. 

* RBIT exists in this arch

* aarch 64 is different than I realized

http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.den0024a/CEGDJGGC.html

Writes of 64 bits or less to a vector register result in the higher bits being zeroed.

A new set of vector reduction operations provide across-lane sum, minimum and maximum.

Floating-point FCSEL and Select and Compare instructions, equivalent to the integer CSEL and CCMP have been added.

Zero register"


xzr wzr

* NEON aarch64 sets the core conditional flags now

art of the main instruction set / no longer optional
■ Set the core condition flags (NZCV) rather than their own
■ Easier to mix control and data flow with NEON


file:///home/d/Downloads/Porting%20to%20ARM%2064-bit.pdf

* I wonder at what point inlining stops being useful

I am under the impression it's an 8 stage pipeline

memset(a,0,16) str(xor(a,a); two instructions, 32? 

* Other identities

I wonder if a pre-compiler for the filtering language is possible
in general?

# More aggressive filtering ideas as a baseline for the campus network
# I have long been filtering out hip, and me doing stress experiments
# on fc::/8 fill me with terror if something accidentally escapes.

# IPv4 Section

# My network is on 20-23

in ip 172.20.0.0/14 ge 14 allow
in ip 172.16.0.0/12 ge 12 deny

# ignore self assigned ips - I get these from boxes that didn't quite
# get their wifi up in time, or after a crash somewhere else

in ip 169.254.0.0/16 ge 16 deny

# No other rfc19xx addresses allowed

in ip 192.168.0.0/16 ge 16 deny # no 192.168 on my network
in ip 10.0.0.0/8 ge 8 deny # no 10

out ip 169.254.0.0/16 ge 16 deny # Don't export any either
# FIXME - can be more aggressive about not exporting them.

# IPv6 section

# my management lan is on this
in ip fd99::/64 ge 64 allow

# My internal ULAs - le 64 is accepted syntax but didn't work
#
in ip fd5d:1c42:b40d::/48 ge 48 # le 64 allow

# in ip fd5d:1c42:b40d:fffc:/62 ge 64 allow # My internal meshy networks
# in ip fd5d:1c42:b40d:fffa:/64 ge 64 allow # internal anycast dns

#in ip fdXX:YY::/48 ge 48 allow  # my new management lan is on this
#in ip never:you:mind::/60 # my vpn tunnel is on this
#in ip never:you:mind2::/60 # my other tunnel is on this

in ip 2601::/20 ge 20 allow     # Comcast supplies dynamic ipv6 addresses
                  # And I never know what they will hand me
                  # next. I get 56, 60s, 64s
                  # Presently the patter is 2601:646:
#in ip 2601:646::/34 ?

in ip fc::/8 ge 8 deny # I use fc for testing stuff
in ip fd::/8 ge 8 deny # I am otherwise perpetually passing accidental v6 around

# Block invalid ipv6 routes

in ip 2001:db8::/32 ge 32 deny
in ip 2002::/16 ge 16 deny
in ip 2001::/32 ge 32 deny # teredo
in ip 2001:10::/28 ge 28 deny # orchid this bugged me majorly once. HIP.
# in ip ff00::/8 ge 8 deny # just because

# Two VPNS - well these were originally 6to4 hurricane tunnels

# Virtual machines
# (Need to put up the risc-v box somewhere)

# Source specific default routes allowed
# src-prefix ::0/

# Never allow a non-vpn address to talk to a vpn address

# Do allow consistent tunneling

# Special stuff (vpn edges)

# Self section

in ip 0.0.0.0/0 eq 0 allow # allow a default route

# allow src-specific default routes only
# FIXME
# Fixme export these for RA as regular routes

# I now inject the "right" addresses from address assignement and lede
# via the new proto facility

redistribute proto 44 #  metric 2048 # Tell the world about the ips I have
redistribute local deny # Don't tell it about other ips I have

# Don't allow in any other addresses to the real world
# Don't allow spoofing 75.75.75.75 etc
# Rely on default routes to get to the real world.
# This minimizes the damage a rogue router announcing a default route to
# the area around it

in deny

* Test idea

Address to BGP route table lookup

* printf

union Vec4 {
    v4sf v;
    float e[4];
    int i[4];
    uint u[4];
    uchar c[16];
};

Vec4 vec;
vec.v = (v4sf){0.1f,0.2f,0.3f,0.4f};
printf("%f %f %f %f\n", vec.e[0], vec.e[1], vec.e[2], vec.e[3]);

* It's coming back to me.

What I'd ended up doing was writing a custom gdb parser that let me 
consistently see regs in whatever format I was working in.

And a reference program that showed all the types that I could objdump
to see. 

And the equivalent of printf to dump the regs at any point, in any format.

and a bit of masm-like building blocks to get zeros, ones, etc. I remember
being unhappy I couldn't specify in the pre-processor a random reg to 
xor....

As for neon...

I think you can load 256 bytes at a time, and rotate them on the way in,
but I'm not sure.

I also need aligned and unaligned variants of the loads/stores. And 
a clean way to get stuff to the main processor answering various questions
(gt/lt/eq/neq). There also seem to be single instructions for nand nxor,
etc....

in the end I'd also basically just invented my own subset of the 
sse related language (sort of how I just did with usimd) for my purposes. 

This time, tho, I'm not doing fp, just boolean operations. I'm a little confused
on shifts and the ABI. If I want to shift all of a vector right what happens?
Do I load the shift to all lanes and go? What happens to the highest value.

Do I want to use the 8 bit types? Mildly easier to conceptualize in some ways.

I wish I could tell objdump to talk about loads as if they were 128 bit
quantities into q registers, in particular.

oh, wow.

http://www.cs.utexas.edu/users/mckinley/papers/js-simd-pact-2015.pdf

* using 8 bit char types might be easier than 64 bit ones

The types defined in this manner can be used with a subset of normal C operations. Currently, GCC allows using the following operators on these types: +, -, *, /, unary minus, ^, |, &, ~, %.


This however is more tricky

Vector comparison is supported with standard comparison operators: ==, !=, <, <=, >, >=. Comparison operands can be vector expressions of integer-type or real-type. Comparison between integer-type vectors and real-type vectors are not supported. The result of the comparison is a vector of the same width and number of elements as the comparison operands with a signed integral element type.



* http://www.catb.org/esr/structure-packing/


git://git.kernel.org/pub/scm/devel/pahole/pahole.git

The clang compiler has a -Wpadded option that causes it to generate messages about alignment holes and padding. Some versions also have an undocumented -fdump-record-layouts option that yields more information.


* The gcc 4.9.2 compiler never issues neon instructions

Even on the simplest things - like memcpy.

Which babel calls memcpy with constants almost exclusively

memzero also

Using my current "optimized" neon build I call vld 80 times:

on the arm-thumb based chip, binary size is:

|size|-O3|-O3 neon|optimized_memcpy|use64bit
|size|112824|112836|113028|113022|

But we go from 80 vlds to 230. My take on the size difference above
is that we fool the register allocator less.

Non-thumb went to 116732 from 116720 so that's good...

With the new code... we ended up with: 233 vlds (??) and 116732

* I find the lvd1.32 d16,d17 vs the q registers really hard to read

I wonder if I can get something that does q

        vld1.32 {d18-d19}, [r2] @ D.40374, MEM[(const __builtin_neon_si[4] *)_19]
        bne     .L373   @,
        ldrb    r5, [r2, #33]   @ zero_extendqisi2      @ _26->src_plen, _26->src_plen
        cmp     r5, r3  @ _26->src_plen, src_plen
        bne     .L373   @,
        veor    q8, q8, q10     @ D.40374, D.40374, D.40374

* Shift 128 bits right by a constant or two sets of ??

uint64x2_t vshrq_n_u64(uint64x2_t a, __constrange(1,64) int b); // VSHR.U64 q0,q0,#64

Left

uint64x2_t vshlq_n_u64(uint64x2_t a, __constrange(0,63) int b); // VSHL.I64 q0,q0,#0

Of course one of the cool things about this is you can shift
by any qty and any subdivisions left or right 

uint8x16_t vshlq_n_u8(uint8x16_t a, __constrange(0,7) int b);   // VSHL.I8 q0,q0,#0 

* Vector narrowing shift right by constant

int8x8_t   vshrn_n_s16(int16x8_t a, __constrange(1,8) int b);   // VSHRN.I16 d0,q0,#8 
int16x4_t  vshrn_n_s32(int32x4_t a, __constrange(1,16) int b);  // VSHRN.I32 d0,q0,#16
int32x2_t  vshrn_n_s64(int64x2_t a, __constrange(1,32) int b);  // VSHRN.I64 d0,q0,#32
uint8x8_t  vshrn_n_u16(uint16x8_t a, __constrange(1,8) int b);  // VSHRN.I16 d0,q0,#8 
uint16x4_t vshrn_n_u32(uint32x4_t a, __constrange(1,16) int b); // VSHRN.I32 d0,q0,#16
uint32x2_t vshrn_n_u64(uint64x2_t a, __constrange(1,32) int b); // VSHRN.I64 d0,q0,#32 

* The saturating shifts look interesting.

* Then there are lanes. You can't get there

uint8x16_t  vld1q_lane_u8(__transfersize(1) uint8_t const * ptr, uint8x16_t vec,
              __constrange(0,15) int lane);                   // VLD1.8 {d0[0]}, [r0]

uint64x2_t  vld1q_lane_u64(__transfersize(1) uint64_t const * ptr, uint64x2_t vec, 
              __constrange(0,1) int lane);                    // VLD1.64 {d0}, [r0]

* shift

uint64x2_t vshlq_u64(uint64x2_t a, int64x2_t b);  // VSHL.U64 q0,q0,q0

So what I basically want to do is load a plen into the bottommost part of something

create a mask

xor(a,a); zeros
not(xor(a,a)) ones



store

* C11 has generics now that might make for less headaches

get_ones() { return vmovq_n_u8 (255) ; } // probabl not(xor(a,a)) faster

uint8x16_t  vmovq_n_u8(uint8_t value);     // VDUP.8 q0,r0 


* Another way to load ones

uint8x16_t  vmovq_n_u8(uint8_t value);     // VDUP.8 q0,r0 

* Big to little endian bit conversion

uint32x4_t   vrev64q_u32(uint32x4_t vec);  // VREV64.32 q0,q0

uint8x16_t   vrev64q_u8(uint8x16_t vec);   // VREV64.8 q0,q0 

* Cool - Popcount capability here


uint8x8_t  vcnt_u8(uint8x8_t a);    // VCNT.8 d0,d0
int8x8_t   vcnt_s8(int8x8_t a);     // VCNT.8 d0,d0
poly8x8_t  vcnt_p8(poly8x8_t a);    // VCNT.8 d0,d0
uint8x16_t vcntq_u8(uint8x16_t a);  // VCNT.8 q0,q0
int8x16_t  vcntq_s8(int8x16_t a);   // VCNT.8 q0,q0
poly8x16_t vcntq_p8(poly8x16_t a);  // VCNT.8 q0,q0

* Bitwise NOT. Grump.

Despite all the strong typechecking here, there is no
uint64x2 version. and it decodes down to the same instruction.

uint32x4_t vmvnq_u32(uint32x4_t a);  // VMVN q0,q0

* Bit Clear - not sure what it does

* Bitwise OR complement

* Casting shit sucks

Syntax
vreinterpret{q}_dsttype_srctype
Where:
q
Specifies that the conversion operates on 128-bit vectors. If it is not present, the conversion operates on 64-bit vectors.
dsttype
Represents the type to convert to.
srctype
Represents the type being converted.
Examples
The following intrinsic reinterprets a vector of four signed 16-bit integers as a vector of four unsigned integers:
uint16x4_t vreinterpret_u16_s16(int16x4_t a);
The following intrinsic reinterprets a vector of four 32-bit floating point values integers as a vector of four signed integers.
int8x16_t vreinterpretq_s8_f32(float32x4_t a);

* 

VMRS
VMSR
Use the named register variable __asm("fpscr") to access the Floating-Point Status and Control Register (FPSCR). Using the named register variable causes the compiler to emit VMRS/VMSR instructions as required.

Go looking for these being called

VPOP
VPUSH

* I'm not confident I got this right elsewhere

uint64_t  vgetq_lane_u64(uint64x2_t vec, __constrange(0,1) int lane);

* Set a single lane from a literal

int32x4_t   vsetq_lane_s32(int32_t value, int32x4_t vec,  
              __constrange(0,3) int lane);                    // VMOV.32 d0[0],r0

so, for example, to construct a v4mapped prefix

load zeros via xor...

(or does the trailing thing need to be ones)

neon_v4prefix = vsetq_lane_s32(hben(0xffff),xor(a,a),1)

neon_v4_mask = (0xffff,0xffff,0xffff,0xffff,0xffff,0xffff,0xffff,0xffff,0,0)

to compare it

xor(and(cprefix,neon_v4mask),neon_v4prefix)

* Let's go back to the original bit

!v6mapped(p1) && !v6mapped(p2);

a = xor(and(p1,neon_v4mask),neon_v4prefix)
b = xor(and(p2,neon_v4mask),neon_v4prefix)
t = or(a,b)

> 96
> 96

* Need to replace memcmp

But most of the time they won't be equal and we
need to find out how they are not.

Greater than/less than 

xor the two together
 eq == 0 
 otherwise, a mess
 reverse each on 8 bit qtys (before or after?)
 ffs bit in both or is it fls?
 and get out all the bits you don't care about in both
 somehow get it back into 8 bits?
 0 = 0 
 shift right to make it signed (and -2? or -1)
 get the lane you were dealing with
 signed bit = shifted 11 - -1? 
 signed bit = 10 shifted = 1
 
* Are we done yet?
  No, we can lift most of this horrific bit of bit twiddling out of the inner
  loop also for the static comparison



* NEON dual issue

The NEON engine has limited dual issue capabilities. A load/store, permute, MCR, or MRC type instruction can be dual issued with a NEON data-processing instruction. A load/store, permute, MCR, or MRC executes in the NEON load/store permute pipeline. A NEON data-processing instruction executes in the NEON integer ALU, Shift, MAC, floating-point add or multiply pipelines. This is the only dual issue pairing permitted.
There are also similar restrictions to the ARM integer pipeline in terms of dual issue pairing with multi-cycle instructions. The NEON engine can potentially dual issue on both the first and last cycle of a multi-cycle instruction, but not on any of the intermediate cycles.

** Checkme - are we writing zeros or the v6 prefix

** Checkme - the arm can have up to 8(?) prefetch operations outstanding

	5 is the defuault for aarch64 - 8 allowed.
	The A7 supports 3 - 3 is the default

** Not clear what builtin_prefech maps to in arm

	The A9 has a single bit for L1 prefetch enable
	and A single bit for L2 prefetch hint enable

** Fascinating extra options

http://stackoverflow.com/questions/16032202/how-to-use-pld-instruction-in-arm
This is eanbled on -O3

 __attribute__((optimize("prefetch-loop-arrays")))

The value of addr is the address of the memory to prefetch. There are two optional arguments, rw and locality. The value of rw is a compile-time constant one or zero; one means that the prefetch is preparing for a write to the memory address and zero, the default, means that the prefetch is preparing for a read. The value locality must be a compile-time constant integer between zero and three. A value of zero means that the data has no temporal locality, so it need not be left in the cache after the access. A value of three means that the data has a high degree of temporal locality and should be left in all levels of cache possible. Values of one and two mean, respectively, a low or moderate degree of temporal locality. The default is three.

     for (i = 0; i < n; i++)
       {
         a[i] = a[i] + b[i];
         __builtin_prefetch (&a[i+j], 1, 1);
         __builtin_prefetch (&b[i+j], 0, 1);
         /* ... */
       }
Data prefetch does not generate faults if addr is invalid, but the address expression itself must be valid. For example, a prefetch of p->next will not fault if p->next is not a valid address, but evaluation will fault if p is not a valid address.

If the target does not support data prefetch, the address expression is evaluated if it includes side effects but no other code is generated and GCC does not issue a warning.

** This had some *GREAT* info on perf and also optimized a binary

http://stackoverflow.com/questions/7327994/prefetching-examples

 $ gcc c-binarysearch.c -DDO_PREFETCH -o with-prefetch -std=c11 -O3
 $ gcc c-binarysearch.c -o no-prefetch -std=c11 -O3

 $ perf stat -e L1-dcache-load-misses,L1-dcache-loads ./with-prefetch 

  Performance counter stats for './with-prefetch':

    356,675,702      L1-dcache-load-misses     #   41.39% of all L1-dcache hits  
   861,807,382      L1-dcache-loads                                             

   8.787467487 seconds time elapsed

 $ perf stat -e L1-dcache-load-misses,L1-dcache-loads ./no-prefetch 

 Performance counter stats for './no-prefetch':

   382,423,177      L1-dcache-load-misses     #   97.36% of all L1-dcache hits  
   392,799,791      L1-dcache-loads                                             

  11.376439030 seconds time elapsed


** Finally, ARMs NEON also offers VCLS (Vector Count Leading Sign Bits), which (quoting from the documentation) “counts the number of consecutive bits following the topmost bit, that are the same as the topmost bit”. Well, we can do that on all architectures I mentioned as well, using only ingredients we already have: arm_cls(x) = x86_lzcnt(x ^ (x >> 1)) - 1 (the shift here is an arithmetic shift). The expression y = x ^ (x >> 1) gives a value that has bit n set if and only if bits n and n + 1 of x are the same. By induction, the number of leading zeros in y is thus exactly the number of leading bits in x that match the sign bit. This count includes the topmost (sign) bit, so it’s always at least 1, and the instruction definition I just quoted requires us to return the number of bits following the topmost bit that match it. So we subtract 1 to get the right result. Since we can do a fast leading zero count on all quoted platforms, we’re good.



For mask

http://stackoverflow.com/questions/23633481/optimizing-mask-function-with-arm-simd-instructions
	
I would probably do it like this. I've included 4x loop unrolling. Preloading the cache is always a good idea and can speed things up another 25%. Since there's not much processing going on (it's mostly spending time loading and storing), it's best to load lots of registers, then process them as it gives time for the data to actually load. It assumes the data is an even multiple of 16 elements.

void fmask(unsigned int x, unsigned int y, uint32_t *s, uint32_t *m)
{                             
  unsigned int ixy;
  uint32x4_t srcA,srcB,srcC,srcD;
  uint32x4_t maskA,maskB,maskC,maskD;

  ixy = xsize * ysize;
  ixy /= 16; // process 16 at a time
  while (ixy--)
  {
    __builtin_prefetch(&s[64]); // preload the cache
    __builtin_prefetch(&m[64]);
    srcA = vld1q_u32(&s[0]);
    maskA = vld1q_u32(&m[0]);
    srcB = vld1q_u32(&s[4]);
    maskB = vld1q_u32(&m[4]);
    srcC = vld1q_u32(&s[8]);
    maskC = vld1q_u32(&m[8]);
    srcD = vld1q_u32(&s[12]);
    maskD = vld1q_u32(&m[12]);
    srcA = vandq_u32(srcA, maskA); 
    srcB = vandq_u32(srcB, maskB); 
    srcC = vandq_u32(srcC, maskC); 
    srcD = vandq_u32(srcD, maskD);
    vst1q_u32(&s[0], srcA);
    vst1q_u32(&s[4], srcB);
    vst1q_u32(&s[8], srcC);
    vst1q_u32(&s[12], srcD);
    s += 16;
    m += 16;
  }
}
